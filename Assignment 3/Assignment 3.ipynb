{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Handle necessary imports\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "pd.set_option('max_colwidth', 500)\n",
    "pd.set_option('max_columns', 500)\n",
    "pd.set_option('max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, we load the IMDB movie dataset. Since all reviews are in seperate\n",
    "#.txt files, first we read them all in and add them to two lists. Then we create two dataframes \n",
    "#(1 for positive, 1 for negative),In which we have two columns: Reviews with review content, and sentiment with sentiment \n",
    "# value (1=positive, 0=negative).We repeat this process twice, once for the train data and one for the test data. \n",
    "# We save the results to csv files, so we don't ever have to re-do the process. \n",
    "# Then we create one big data-frame by appending the train dataframe to the test dataframe. \n",
    "pos_test_files = os.listdir(\"C:/Users/Yme/Documents/MEGA/Master DSE/Data Mining/Assignments/Assignment 3/data/aclImdb/test/pos\")\n",
    "neg_test_files = os.listdir(\"C:/Users/Yme/Documents/MEGA/Master DSE/Data Mining/Assignments/Assignment 3/data/aclImdb/test/neg\")\n",
    "pos_train_files = os.listdir(\"C:/Users/Yme/Documents/MEGA/Master DSE/Data Mining/Assignments/Assignment 3/data/aclImdb/train/pos\")\n",
    "neg_train_files = os.listdir(\"C:/Users/Yme/Documents/MEGA/Master DSE/Data Mining/Assignments/Assignment 3/data/aclImdb/train/neg\")\n",
    "\n",
    "train_pos_list = []\n",
    "for file in pos_train_files:\n",
    "    with open(\"C:/Users/Yme/Documents/MEGA/Master DSE/Data Mining/Assignments/Assignment 3/data/aclImdb/train/pos/{}\".format(file), 'r', encoding='utf-8') as fd:\n",
    "        text = fd.read()\n",
    "        train_pos_list.append(text)\n",
    "\n",
    "train_neg_list = []\n",
    "for file in neg_train_files:\n",
    "    with open(\"C:/Users/Yme/Documents/MEGA/Master DSE/Data Mining/Assignments/Assignment 3/data/aclImdb/train/neg/{}\".format(file), 'r',encoding='utf-8') as fd:\n",
    "        text = fd.read()\n",
    "        train_neg_list.append(text)\n",
    "\n",
    "train_data_pos = {'label' : 1, 'text': train_pos_list}\n",
    "train_data_neg = {'label' :0, 'text': train_neg_list}\n",
    "train_data_frame_complete = pd.DataFrame(train_data_pos).append(pd.DataFrame(train_data_neg)).reset_index(drop=True)\n",
    "train_data_frame_complete.to_csv('imdb_train.csv', index=False)\n",
    "\n",
    "test_pos_list = []\n",
    "for file in pos_test_files:\n",
    "    with open(\"C:/Users/Yme/Documents/MEGA/Master DSE/Data Mining/Assignments/Assignment 3/data/aclImdb/test/pos/{}\".format(file), 'r', encoding='utf-8') as fd:\n",
    "        text = fd.read()\n",
    "        test_pos_list.append(text)\n",
    "\n",
    "test_neg_list = []\n",
    "for file in neg_test_files:\n",
    "    with open(\"C:/Users/Yme/Documents/MEGA/Master DSE/Data Mining/Assignments/Assignment 3/data/aclImdb/test/neg/{}\".format(file), 'r',encoding='utf-8') as fd:\n",
    "        text = fd.read()\n",
    "        test_neg_list.append(text)\n",
    "\n",
    "test_data_pos = {'label' : 1, 'text': test_pos_list}\n",
    "test_data_neg = {'label' :0, 'text': test_neg_list}\n",
    "test_data_frame_complete = pd.DataFrame(test_data_pos).append(pd.DataFrame(test_data_neg)).reset_index(drop=True)\n",
    "test_data_frame_complete.to_csv('imdb_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>26247</td>\n",
       "      <td>1</td>\n",
       "      <td>Fame is one of the best movies I've seen about The Performing Arts. The music and the acting are excellent. The screenplay and Set Design are also excellent. My favorite part is when all the students start Dancing and making music in the Canteen. I can see this movie any number of times, and never get bored. I give it 8 1/2 on 10.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35067</td>\n",
       "      <td>1</td>\n",
       "      <td>This movie fully deserves to be one of the top Hindi comedies ever made . Rajkumar Santoshi is mostly known for his gritty hard-hitting social dramas , but this is easily the most effortless movie he has made .&lt;br /&gt;&lt;br /&gt;The plot revolves around two small-town buffoons Amar (Aamir Khan) and Prem (Salman Khan) . They want to get rich quick and so move to the big city . They hatch the same plan separately - to woo a rich heiress Raveena (Raveena Tandon) who is the daughter of a rich businessm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34590</td>\n",
       "      <td>1</td>\n",
       "      <td>in a time of predictable movies, in which abound violence, cheap romance and melodrama, it is delightfully surprising to find such a strange movie. the plot itself is compelling, and the actors are excellent, especially Alan Rickman. If you want to watch a movie that does not provide all the answers before asking the questions, a movie that will surprise you (in good or bad), Dark Harbor's for you. And if you're not convinced, believe me that Alan Rickman's performance is well worth it... es...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label  \\\n",
       "26247      1   \n",
       "35067      1   \n",
       "34590      1   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \n",
       "26247                                                                                                                                                                         Fame is one of the best movies I've seen about The Performing Arts. The music and the acting are excellent. The screenplay and Set Design are also excellent. My favorite part is when all the students start Dancing and making music in the Canteen. I can see this movie any number of times, and never get bored. I give it 8 1/2 on 10.  \n",
       "35067  This movie fully deserves to be one of the top Hindi comedies ever made . Rajkumar Santoshi is mostly known for his gritty hard-hitting social dramas , but this is easily the most effortless movie he has made .<br /><br />The plot revolves around two small-town buffoons Amar (Aamir Khan) and Prem (Salman Khan) . They want to get rich quick and so move to the big city . They hatch the same plan separately - to woo a rich heiress Raveena (Raveena Tandon) who is the daughter of a rich businessm...  \n",
       "34590  in a time of predictable movies, in which abound violence, cheap romance and melodrama, it is delightfully surprising to find such a strange movie. the plot itself is compelling, and the actors are excellent, especially Alan Rickman. If you want to watch a movie that does not provide all the answers before asking the questions, a movie that will surprise you (in good or bad), Dark Harbor's for you. And if you're not convinced, believe me that Alan Rickman's performance is well worth it... es...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Then we load all data, and create one big csv file. This csv file has the first 25k rows as \"train data\", and \n",
    "#the last 25k rows as \"test data\". Of course, this division is arbitrary (It was the way the dataset was divided when downloaded),\n",
    "#and we can specify our own train/test ratio later on. \n",
    "# df_imdb_train = pd.read_csv(\"imdb_train.csv\")\n",
    "# df_imdb_test = pd.read_csv(\"imdb_test.csv\")\n",
    "# imdb_final = df_imdb_train.append(df_imdb_test, ignore_index=True)\n",
    "# imdb_final.to_csv(\"imdb_final.csv\", index=False)\n",
    "\n",
    "# Print some information to check if all went well. We still have to do some pre-processing.\n",
    "#We see that there is still some HTML tags in the data. Also,there is some punctuation, and special characters.\n",
    "#Additional pre-processing will be needed. \n",
    "imdb_final = pd.read_csv(\"imdb_final.csv\")\n",
    "display(imdb_final.sample(3, random_state=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>374696</td>\n",
       "      <td>1</td>\n",
       "      <td>@PinkBerryGirl Level 16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770756</td>\n",
       "      <td>1</td>\n",
       "      <td>heyy ,, i'm listening to the kind of music , i go to camping tonight , with friends .. so cool   hey miley i love the season 3 of  hm !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630152</td>\n",
       "      <td>0</td>\n",
       "      <td>@beardoctor Removal people are due Friday &amp;amp; there's work to be done. But we're still on the net. - Same here  But our boxes haven't come</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>865943</td>\n",
       "      <td>0</td>\n",
       "      <td>@jasonhilimire no, not today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1518600</td>\n",
       "      <td>1</td>\n",
       "      <td>Our for the weekend!  Check out my blog, and let me know if you would like to guest post on it  http://www.brenelz.com/blog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1567427</td>\n",
       "      <td>0</td>\n",
       "      <td>when's my new iphone gonna get here. le sigh</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         label  \\\n",
       "374696       1   \n",
       "770756       1   \n",
       "630152       0   \n",
       "865943       0   \n",
       "1518600      1   \n",
       "1567427      0   \n",
       "\n",
       "                                                                                                                                                 text  \n",
       "374696                                                                                                                       @PinkBerryGirl Level 16   \n",
       "770756        heyy ,, i'm listening to the kind of music , i go to camping tonight , with friends .. so cool   hey miley i love the season 3 of  hm !  \n",
       "630152   @beardoctor Removal people are due Friday &amp; there's work to be done. But we're still on the net. - Same here  But our boxes haven't come  \n",
       "865943                                                                                                                  @jasonhilimire no, not today   \n",
       "1518600                   Our for the weekend!  Check out my blog, and let me know if you would like to guest post on it  http://www.brenelz.com/blog  \n",
       "1567427                                                                                                 when's my new iphone gonna get here. le sigh   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Then, we load in Twitter data. The dataset we used is a \"dumbed down/light version\" of the Sentiment140 dataset that was \n",
    "#provided in the link you gave us. http://help.sentiment140.com/ is the source of the original dataset; and it can be found \n",
    "#https://www.kaggle.com/kazanova/sentiment140 here in its full form. However, we stumbled upon a more suitable version of the dataset\n",
    "#here: https://www.kaggle.com/ywang311/twitter-sentiment. This dataset doesn't have all the columns we do not need.\n",
    "#Since we want to compare the IMDB movie ratings sentiments with the twitter sentiment, we filter all tweets about movies.  \n",
    "twitter = pd.read_csv(\"C:/Users/Yme/Documents/MEGA/Master DSE/Data Mining/Assignments/Assignment 3/data/Sentiment Analysis Dataset 2.csv\", skiprows=[8835,535881] , usecols = ['Sentiment' , 'SentimentText'])\n",
    "twitter = twitter.rename(columns = {'Sentiment': 'label' , 'SentimentText':'text'})\n",
    "display(twitter.sample(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>26247</td>\n",
       "      <td>1</td>\n",
       "      <td>fame one best movies seen performing arts music acting excellent screenplay set design also excellent favorite part students start dancing making music canteen see movie number times never get bored give 8 1 2 10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35067</td>\n",
       "      <td>1</td>\n",
       "      <td>movie fully deserves one top hindi comedies ever made rajkumar santoshi mostly known gritty hard hitting social dramas easily effortless movie made br br plot revolves around two small town buffoons amar aamir khan prem salman khan want get rich quick move big city hatch plan separately woo rich heiress raveena raveena tandon daughter rich businessman ramgopal bajaj paresh rawal thus one marries raveena gets hands wealth get know plan intense tussle one oneupmanship marries raveena hilarious...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34590</td>\n",
       "      <td>1</td>\n",
       "      <td>time predictable movies abound violence cheap romance melodrama delightfully surprising find strange movie plot compelling actors excellent especially alan rickman want watch movie provide answers asking questions movie surprise good bad dark harbor convinced believe alan rickman performance well worth especially end ladies</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label  \\\n",
       "26247      1   \n",
       "35067      1   \n",
       "34590      1   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \n",
       "26247                                                                                                                                                                                                                                                                                                 fame one best movies seen performing arts music acting excellent screenplay set design also excellent favorite part students start dancing making music canteen see movie number times never get bored give 8 1 2 10  \n",
       "35067  movie fully deserves one top hindi comedies ever made rajkumar santoshi mostly known gritty hard hitting social dramas easily effortless movie made br br plot revolves around two small town buffoons amar aamir khan prem salman khan want get rich quick move big city hatch plan separately woo rich heiress raveena raveena tandon daughter rich businessman ramgopal bajaj paresh rawal thus one marries raveena gets hands wealth get know plan intense tussle one oneupmanship marries raveena hilarious...  \n",
       "34590                                                                                                                                                                                time predictable movies abound violence cheap romance melodrama delightfully surprising find strange movie plot compelling actors excellent especially alan rickman want watch movie provide answers asking questions movie surprise good bad dark harbor convinced believe alan rickman performance well worth especially end ladies  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1564943</td>\n",
       "      <td>1</td>\n",
       "      <td>well time bed 5 00 comes early nice chatting everyone good evening rest weekend left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1287022</td>\n",
       "      <td>1</td>\n",
       "      <td>defyingsantafe umm forget gay socialist atheists</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1526836</td>\n",
       "      <td>0</td>\n",
       "      <td>mom nearly got ran truck bike dropped work bag information stolen fb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         label  \\\n",
       "1564943      1   \n",
       "1287022      1   \n",
       "1526836      0   \n",
       "\n",
       "                                                                                         text  \n",
       "1564943  well time bed 5 00 comes early nice chatting everyone good evening rest weekend left  \n",
       "1287022                                      defyingsantafe umm forget gay socialist atheists  \n",
       "1526836                  mom nearly got ran truck bike dropped work bag information stolen fb  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Remove link, user, special characters and stopwords.\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "#Please note that this somewhat complex regex was found online... First we did\n",
    "# a lot of the pre-processing manually, but then we stumbled upon this regex \n",
    "#And are very happy that smart people took care of building this regex for us =)\n",
    "TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
    "\n",
    "def preprocess(text):\n",
    "    text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n",
    "    tokens = []\n",
    "    for token in text.split():\n",
    "        if token not in stop_words:\n",
    "                tokens.append(token)\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "imdb_final['text'] = imdb_final['text'].apply(lambda x: preprocess(x))\n",
    "display(imdb_final.sample(3, random_state=1))\n",
    "\n",
    "twitter['text'] = twitter['text'].apply(lambda x: preprocess(x))\n",
    "display(twitter.sample(3, random_state=1))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>text_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1564943</td>\n",
       "      <td>1</td>\n",
       "      <td>well time bed 5 00 comes early nice chatting everyone good evening rest weekend left</td>\n",
       "      <td>[well, time, bed, 5, 00, comes, early, nice, chatting, everyone, good, evening, rest, weekend, left]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1287022</td>\n",
       "      <td>1</td>\n",
       "      <td>defyingsantafe umm forget gay socialist atheists</td>\n",
       "      <td>[defyingsantafe, umm, forget, gay, socialist, atheists]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1526836</td>\n",
       "      <td>0</td>\n",
       "      <td>mom nearly got ran truck bike dropped work bag information stolen fb</td>\n",
       "      <td>[mom, nearly, got, ran, truck, bike, dropped, work, bag, information, stolen, fb]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         label  \\\n",
       "1564943      1   \n",
       "1287022      1   \n",
       "1526836      0   \n",
       "\n",
       "                                                                                         text  \\\n",
       "1564943  well time bed 5 00 comes early nice chatting everyone good evening rest weekend left   \n",
       "1287022                                      defyingsantafe umm forget gay socialist atheists   \n",
       "1526836                  mom nearly got ran truck bike dropped work bag information stolen fb   \n",
       "\n",
       "                                                                                               text_tokenized  \n",
       "1564943  [well, time, bed, 5, 00, comes, early, nice, chatting, everyone, good, evening, rest, weekend, left]  \n",
       "1287022                                               [defyingsantafe, umm, forget, gay, socialist, atheists]  \n",
       "1526836                     [mom, nearly, got, ran, truck, bike, dropped, work, bag, information, stolen, fb]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>text_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>26247</td>\n",
       "      <td>1</td>\n",
       "      <td>fame one best movies seen performing arts music acting excellent screenplay set design also excellent favorite part students start dancing making music canteen see movie number times never get bored give 8 1 2 10</td>\n",
       "      <td>[fame, one, best, movies, seen, performing, arts, music, acting, excellent, screenplay, set, design, also, excellent, favorite, part, students, start, dancing, making, music, canteen, see, movie, number, times, never, get, bored, give, 8, 1, 2, 10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35067</td>\n",
       "      <td>1</td>\n",
       "      <td>movie fully deserves one top hindi comedies ever made rajkumar santoshi mostly known gritty hard hitting social dramas easily effortless movie made br br plot revolves around two small town buffoons amar aamir khan prem salman khan want get rich quick move big city hatch plan separately woo rich heiress raveena raveena tandon daughter rich businessman ramgopal bajaj paresh rawal thus one marries raveena gets hands wealth get know plan intense tussle one oneupmanship marries raveena hilarious...</td>\n",
       "      <td>[movie, fully, deserves, one, top, hindi, comedies, ever, made, rajkumar, santoshi, mostly, known, gritty, hard, hitting, social, dramas, easily, effortless, movie, made, br, br, plot, revolves, around, two, small, town, buffoons, amar, aamir, khan, prem, salman, khan, want, get, rich, quick, move, big, city, hatch, plan, separately, woo, rich, heiress, raveena, raveena, tandon, daughter, rich, businessman, ramgopal, bajaj, paresh, rawal, thus, one, marries, raveena, gets, hands, wealth, get...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34590</td>\n",
       "      <td>1</td>\n",
       "      <td>time predictable movies abound violence cheap romance melodrama delightfully surprising find strange movie plot compelling actors excellent especially alan rickman want watch movie provide answers asking questions movie surprise good bad dark harbor convinced believe alan rickman performance well worth especially end ladies</td>\n",
       "      <td>[time, predictable, movies, abound, violence, cheap, romance, melodrama, delightfully, surprising, find, strange, movie, plot, compelling, actors, excellent, especially, alan, rickman, want, watch, movie, provide, answers, asking, questions, movie, surprise, good, bad, dark, harbor, convinced, believe, alan, rickman, performance, well, worth, especially, end, ladies]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label  \\\n",
       "26247      1   \n",
       "35067      1   \n",
       "34590      1   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
       "26247                                                                                                                                                                                                                                                                                                 fame one best movies seen performing arts music acting excellent screenplay set design also excellent favorite part students start dancing making music canteen see movie number times never get bored give 8 1 2 10   \n",
       "35067  movie fully deserves one top hindi comedies ever made rajkumar santoshi mostly known gritty hard hitting social dramas easily effortless movie made br br plot revolves around two small town buffoons amar aamir khan prem salman khan want get rich quick move big city hatch plan separately woo rich heiress raveena raveena tandon daughter rich businessman ramgopal bajaj paresh rawal thus one marries raveena gets hands wealth get know plan intense tussle one oneupmanship marries raveena hilarious...   \n",
       "34590                                                                                                                                                                                time predictable movies abound violence cheap romance melodrama delightfully surprising find strange movie plot compelling actors excellent especially alan rickman want watch movie provide answers asking questions movie surprise good bad dark harbor convinced believe alan rickman performance well worth especially end ladies   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            text_tokenized  \n",
       "26247                                                                                                                                                                                                                                                             [fame, one, best, movies, seen, performing, arts, music, acting, excellent, screenplay, set, design, also, excellent, favorite, part, students, start, dancing, making, music, canteen, see, movie, number, times, never, get, bored, give, 8, 1, 2, 10]  \n",
       "35067  [movie, fully, deserves, one, top, hindi, comedies, ever, made, rajkumar, santoshi, mostly, known, gritty, hard, hitting, social, dramas, easily, effortless, movie, made, br, br, plot, revolves, around, two, small, town, buffoons, amar, aamir, khan, prem, salman, khan, want, get, rich, quick, move, big, city, hatch, plan, separately, woo, rich, heiress, raveena, raveena, tandon, daughter, rich, businessman, ramgopal, bajaj, paresh, rawal, thus, one, marries, raveena, gets, hands, wealth, get...  \n",
       "34590                                                                                                                                    [time, predictable, movies, abound, violence, cheap, romance, melodrama, delightfully, surprising, find, strange, movie, plot, compelling, actors, excellent, especially, alan, rickman, want, watch, movie, provide, answers, asking, questions, movie, surprise, good, bad, dark, harbor, convinced, believe, alan, rickman, performance, well, worth, especially, end, ladies]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Tokenize the words (This breaks up the strings into a list of words or pieces)\n",
    "twitter['text_tokenized'] = [_text.split() for _text in twitter['text']] \n",
    "imdb_final['text_tokenized'] = [_text.split() for _text in imdb_final['text']] \n",
    "display(twitter.sample(3, random_state=1)) \n",
    "display(imdb_final.sample(3, random_state=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>text_tokenized</th>\n",
       "      <th>is_movie_related</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>871270</td>\n",
       "      <td>0</td>\n",
       "      <td>want watch dora movieeeeee</td>\n",
       "      <td>[want, watch, dora, movieeeeee]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1185304</td>\n",
       "      <td>1</td>\n",
       "      <td>saw speidi today lots amazing car including aston martin new bond movie</td>\n",
       "      <td>[saw, speidi, today, lots, amazing, car, including, aston, martin, new, bond, movie]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1562816</td>\n",
       "      <td>0</td>\n",
       "      <td>watching da vinci code alone appartment</td>\n",
       "      <td>[watching, da, vinci, code, alone, appartment]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91151</td>\n",
       "      <td>1</td>\n",
       "      <td>hey 2 u 2 wanna watch awards lol</td>\n",
       "      <td>[hey, 2, u, 2, wanna, watch, awards, lol]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1057186</td>\n",
       "      <td>1</td>\n",
       "      <td>nvm earphones watched wizards waverly place online lol felt like watching thanks internet</td>\n",
       "      <td>[nvm, earphones, watched, wizards, waverly, place, online, lol, felt, like, watching, thanks, internet]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4058</td>\n",
       "      <td>1</td>\n",
       "      <td>wonderful way waste day watching harold kumar movies</td>\n",
       "      <td>[wonderful, way, waste, day, watching, harold, kumar, movies]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         label  \\\n",
       "871270       0   \n",
       "1185304      1   \n",
       "1562816      0   \n",
       "91151        1   \n",
       "1057186      1   \n",
       "4058         1   \n",
       "\n",
       "                                                                                              text  \\\n",
       "871270                                                                  want watch dora movieeeeee   \n",
       "1185304                    saw speidi today lots amazing car including aston martin new bond movie   \n",
       "1562816                                                    watching da vinci code alone appartment   \n",
       "91151                                                             hey 2 u 2 wanna watch awards lol   \n",
       "1057186  nvm earphones watched wizards waverly place online lol felt like watching thanks internet   \n",
       "4058                                          wonderful way waste day watching harold kumar movies   \n",
       "\n",
       "                                                                                                  text_tokenized  \\\n",
       "871270                                                                           [want, watch, dora, movieeeeee]   \n",
       "1185304                     [saw, speidi, today, lots, amazing, car, including, aston, martin, new, bond, movie]   \n",
       "1562816                                                           [watching, da, vinci, code, alone, appartment]   \n",
       "91151                                                                  [hey, 2, u, 2, wanna, watch, awards, lol]   \n",
       "1057186  [nvm, earphones, watched, wizards, waverly, place, online, lol, felt, like, watching, thanks, internet]   \n",
       "4058                                               [wonderful, way, waste, day, watching, harold, kumar, movies]   \n",
       "\n",
       "         is_movie_related  \n",
       "871270               True  \n",
       "1185304              True  \n",
       "1562816              True  \n",
       "91151                True  \n",
       "1057186              True  \n",
       "4058                 True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Since our goal is to compare tweets about movies and imdb movie reviews, we filter the twitter dataset\n",
    "#on movie-related words. Please note that this list is completely arbitrary, and it is an assumption\n",
    "#that all the tweets which have these words in them are actually about movies.\n",
    "MOVIE_RELATED_WORDS = set([\"movie\", \"movies\", \"watch\", \n",
    "                           \"watching\", \"film\", \"cinema\", \n",
    "                           \"actor\", \"thriller\", \n",
    "                           \"horror\", \"dvd\", \"bluray\", \"soundtrack\", \n",
    "                            \"director\", \"remake\", \"blockbuster\"])\n",
    "\n",
    "def contains_movie_word(words):\n",
    "    return any(word in MOVIE_RELATED_WORDS for word in words)\n",
    "\n",
    "twitter['is_movie_related'] = twitter['text_tokenized'].apply(contains_movie_word)\n",
    "twitter_movie = twitter[twitter['is_movie_related']==True]\n",
    "display(twitter_movie.sample(6, random_state=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add positve and negative labels to the dataframe, for later use in the CNN\n",
    "pos_imdb = []\n",
    "neg_imdb = []\n",
    "for l in imdb_final['label']:\n",
    "    if l == 0:\n",
    "        pos_imdb.append(0)\n",
    "        neg_imdb.append(1)\n",
    "    elif l == 1:\n",
    "        pos_imdb.append(1)\n",
    "        neg_imdb.append(0)\n",
    "\n",
    "pos_tweet = []\n",
    "neg_tweet = []\n",
    "for l in twitter_movie['label']:\n",
    "    if l == 0:\n",
    "        pos_tweet.append(0)\n",
    "        neg_tweet.append(1)\n",
    "    elif l == 1:\n",
    "        pos_tweet.append(1)\n",
    "        neg_tweet.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yme\\Anaconda3\\envs\\DeepLearning3.6\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\Yme\\Anaconda3\\envs\\DeepLearning3.6\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>text_tokenized</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>26247</td>\n",
       "      <td>1</td>\n",
       "      <td>fame one best movies seen performing arts music acting excellent screenplay set design also excellent favorite part students start dancing making music canteen see movie number times never get bored give 8 1 2 10</td>\n",
       "      <td>[fame, one, best, movies, seen, performing, arts, music, acting, excellent, screenplay, set, design, also, excellent, favorite, part, students, start, dancing, making, music, canteen, see, movie, number, times, never, get, bored, give, 8, 1, 2, 10]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35067</td>\n",
       "      <td>1</td>\n",
       "      <td>movie fully deserves one top hindi comedies ever made rajkumar santoshi mostly known gritty hard hitting social dramas easily effortless movie made br br plot revolves around two small town buffoons amar aamir khan prem salman khan want get rich quick move big city hatch plan separately woo rich heiress raveena raveena tandon daughter rich businessman ramgopal bajaj paresh rawal thus one marries raveena gets hands wealth get know plan intense tussle one oneupmanship marries raveena hilarious...</td>\n",
       "      <td>[movie, fully, deserves, one, top, hindi, comedies, ever, made, rajkumar, santoshi, mostly, known, gritty, hard, hitting, social, dramas, easily, effortless, movie, made, br, br, plot, revolves, around, two, small, town, buffoons, amar, aamir, khan, prem, salman, khan, want, get, rich, quick, move, big, city, hatch, plan, separately, woo, rich, heiress, raveena, raveena, tandon, daughter, rich, businessman, ramgopal, bajaj, paresh, rawal, thus, one, marries, raveena, gets, hands, wealth, get...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34590</td>\n",
       "      <td>1</td>\n",
       "      <td>time predictable movies abound violence cheap romance melodrama delightfully surprising find strange movie plot compelling actors excellent especially alan rickman want watch movie provide answers asking questions movie surprise good bad dark harbor convinced believe alan rickman performance well worth especially end ladies</td>\n",
       "      <td>[time, predictable, movies, abound, violence, cheap, romance, melodrama, delightfully, surprising, find, strange, movie, plot, compelling, actors, excellent, especially, alan, rickman, want, watch, movie, provide, answers, asking, questions, movie, surprise, good, bad, dark, harbor, convinced, believe, alan, rickman, performance, well, worth, especially, end, ladies]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label  \\\n",
       "26247      1   \n",
       "35067      1   \n",
       "34590      1   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
       "26247                                                                                                                                                                                                                                                                                                 fame one best movies seen performing arts music acting excellent screenplay set design also excellent favorite part students start dancing making music canteen see movie number times never get bored give 8 1 2 10   \n",
       "35067  movie fully deserves one top hindi comedies ever made rajkumar santoshi mostly known gritty hard hitting social dramas easily effortless movie made br br plot revolves around two small town buffoons amar aamir khan prem salman khan want get rich quick move big city hatch plan separately woo rich heiress raveena raveena tandon daughter rich businessman ramgopal bajaj paresh rawal thus one marries raveena gets hands wealth get know plan intense tussle one oneupmanship marries raveena hilarious...   \n",
       "34590                                                                                                                                                                                time predictable movies abound violence cheap romance melodrama delightfully surprising find strange movie plot compelling actors excellent especially alan rickman want watch movie provide answers asking questions movie surprise good bad dark harbor convinced believe alan rickman performance well worth especially end ladies   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            text_tokenized  \\\n",
       "26247                                                                                                                                                                                                                                                             [fame, one, best, movies, seen, performing, arts, music, acting, excellent, screenplay, set, design, also, excellent, favorite, part, students, start, dancing, making, music, canteen, see, movie, number, times, never, get, bored, give, 8, 1, 2, 10]   \n",
       "35067  [movie, fully, deserves, one, top, hindi, comedies, ever, made, rajkumar, santoshi, mostly, known, gritty, hard, hitting, social, dramas, easily, effortless, movie, made, br, br, plot, revolves, around, two, small, town, buffoons, amar, aamir, khan, prem, salman, khan, want, get, rich, quick, move, big, city, hatch, plan, separately, woo, rich, heiress, raveena, raveena, tandon, daughter, rich, businessman, ramgopal, bajaj, paresh, rawal, thus, one, marries, raveena, gets, hands, wealth, get...   \n",
       "34590                                                                                                                                    [time, predictable, movies, abound, violence, cheap, romance, melodrama, delightfully, surprising, find, strange, movie, plot, compelling, actors, excellent, especially, alan, rickman, want, watch, movie, provide, answers, asking, questions, movie, surprise, good, bad, dark, harbor, convinced, believe, alan, rickman, performance, well, worth, especially, end, ladies]   \n",
       "\n",
       "       Pos  Neg  \n",
       "26247    1    0  \n",
       "35067    1    0  \n",
       "34590    1    0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>text_tokenized</th>\n",
       "      <th>is_movie_related</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>871270</td>\n",
       "      <td>0</td>\n",
       "      <td>want watch dora movieeeeee</td>\n",
       "      <td>[want, watch, dora, movieeeeee]</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1185304</td>\n",
       "      <td>1</td>\n",
       "      <td>saw speidi today lots amazing car including aston martin new bond movie</td>\n",
       "      <td>[saw, speidi, today, lots, amazing, car, including, aston, martin, new, bond, movie]</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1562816</td>\n",
       "      <td>0</td>\n",
       "      <td>watching da vinci code alone appartment</td>\n",
       "      <td>[watching, da, vinci, code, alone, appartment]</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         label  \\\n",
       "871270       0   \n",
       "1185304      1   \n",
       "1562816      0   \n",
       "\n",
       "                                                                            text  \\\n",
       "871270                                                want watch dora movieeeeee   \n",
       "1185304  saw speidi today lots amazing car including aston martin new bond movie   \n",
       "1562816                                  watching da vinci code alone appartment   \n",
       "\n",
       "                                                                               text_tokenized  \\\n",
       "871270                                                        [want, watch, dora, movieeeeee]   \n",
       "1185304  [saw, speidi, today, lots, amazing, car, including, aston, martin, new, bond, movie]   \n",
       "1562816                                        [watching, da, vinci, code, alone, appartment]   \n",
       "\n",
       "         is_movie_related  Pos  Neg  \n",
       "871270               True    0    1  \n",
       "1185304              True    1    0  \n",
       "1562816              True    0    1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imdb_final['Pos']= pos_imdb\n",
    "imdb_final['Neg']= neg_imdb\n",
    "\n",
    "twitter_movie['Pos']= pos_tweet\n",
    "twitter_movie['Neg']= neg_tweet\n",
    "\n",
    "display(imdb_final.sample(3, random_state=1))\n",
    "display(twitter_movie.sample(3, random_state=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For a basic representation to train our first neural network, we use a  CountVectorizer. \n",
    "#This is one of the most \"simple\" text embeddings. In Question 2 we will try another (more suitable) one.\n",
    "#This provides a simple way to tokenize a collection of text  documents and build a vocabulary of known words. \n",
    "#We chose this approach instead of the one-hot encoding you suggested to us. The reason for this is you referred us to\n",
    "#lecture 8, but lecture 8 uses the built in get_word_index() function from the imdb dataset, \n",
    "#which we cannot use for present data. \n",
    "\n",
    "# #Write files to pickle for later use\n",
    "twitter_movie.to_pickle(\"twitter_pre_processed.pkl\")\n",
    "imdb_final.to_pickle(\"imdb_pre_processed.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>text_tokenized</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>48232</td>\n",
       "      <td>0</td>\n",
       "      <td>honestly know begin reviewing movie pathetic ernest goes africa aside two three good laughs dispersed throughout film nothing positive hour half waste time life incredible someone able round group people willing act film edit piece trash even incredible eighth installment ernest series br br opening credits movie see ernest posing next various african objects wooden masks heads african animals making faces gestures would probably make 3rd graders laugh opening scene gives viewer taste ernest...</td>\n",
       "      <td>[honestly, know, begin, reviewing, movie, pathetic, ernest, goes, africa, aside, two, three, good, laughs, dispersed, throughout, film, nothing, positive, hour, half, waste, time, life, incredible, someone, able, round, group, people, willing, act, film, edit, piece, trash, even, incredible, eighth, installment, ernest, series, br, br, opening, credits, movie, see, ernest, posing, next, various, african, objects, wooden, masks, heads, african, animals, making, faces, gestures, would, probabl...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5208</td>\n",
       "      <td>1</td>\n",
       "      <td>film outstanding despite nc 17 rating disturbing scenes reality things like happen movie shows lot starts maya rosario dawson superb performance whose recently started attending college everything going well meets jared chad faust terrific performance frat party turns real gentleman sweet invites dinner look stars bridge end going apartment talk takes basement become flirtatious tries put end rapes incident scars goes club meets bartender dj adrian greatfully played marcus patrick sees getti...</td>\n",
       "      <td>[film, outstanding, despite, nc, 17, rating, disturbing, scenes, reality, things, like, happen, movie, shows, lot, starts, maya, rosario, dawson, superb, performance, whose, recently, started, attending, college, everything, going, well, meets, jared, chad, faust, terrific, performance, frat, party, turns, real, gentleman, sweet, invites, dinner, look, stars, bridge, end, going, apartment, talk, takes, basement, become, flirtatious, tries, put, end, rapes, incident, scars, goes, club, meets,...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26407</td>\n",
       "      <td>1</td>\n",
       "      <td>pleasure seeing saltimbanco live seeing video version show nothing compare actually people behind video amazing job capturing flavor feel sensation much wonderful performances saltimbanco stunningly amazing troupe beautifully captured throughout video flows smoothly artfully production wonderful experience</td>\n",
       "      <td>[pleasure, seeing, saltimbanco, live, seeing, video, version, show, nothing, compare, actually, people, behind, video, amazing, job, capturing, flavor, feel, sensation, much, wonderful, performances, saltimbanco, stunningly, amazing, troupe, beautifully, captured, throughout, video, flows, smoothly, artfully, production, wonderful, experience]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37814</td>\n",
       "      <td>0</td>\n",
       "      <td>think cheaters needs air end reality show care anyone says attack agree times like show spewing propaganda host cheaters joey greco little bastard wants think showing people camera effective unawares show disgusting also wiretapping following people cheaters spy illegal federal offense living police state like soviet union nazi germany rolled one happy poor reviews trash needs end soon going lose liberties nation wonder country going hell filth shows liked older shows better 1950s 1980s hope...</td>\n",
       "      <td>[think, cheaters, needs, air, end, reality, show, care, anyone, says, attack, agree, times, like, show, spewing, propaganda, host, cheaters, joey, greco, little, bastard, wants, think, showing, people, camera, effective, unawares, show, disgusting, also, wiretapping, following, people, cheaters, spy, illegal, federal, offense, living, police, state, like, soviet, union, nazi, germany, rolled, one, happy, poor, reviews, trash, needs, end, soon, going, lose, liberties, nation, wonder, country,...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>709</td>\n",
       "      <td>1</td>\n",
       "      <td>dolph lundgren back detention marks dolphs first film nearly 2 years following delayed hidden agenda film still marks improvement dolph cheapie trilogy jill rips agent red stormcatcher however film well standard hidden agenda better almost every respect film favour dolph previous outing sense cheesy fun film also rejuvenated dolph back high action role good see dolph stunts br br films story ludicrous prime b movie material ex military man teacher last day teaching whilst taking detention cl...</td>\n",
       "      <td>[dolph, lundgren, back, detention, marks, dolphs, first, film, nearly, 2, years, following, delayed, hidden, agenda, film, still, marks, improvement, dolph, cheapie, trilogy, jill, rips, agent, red, stormcatcher, however, film, well, standard, hidden, agenda, better, almost, every, respect, film, favour, dolph, previous, outing, sense, cheesy, fun, film, also, rejuvenated, dolph, back, high, action, role, good, see, dolph, stunts, br, br, films, story, ludicrous, prime, b, movie, material, e...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label  \\\n",
       "48232      0   \n",
       "5208       1   \n",
       "26407      1   \n",
       "37814      0   \n",
       "709        1   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
       "48232  honestly know begin reviewing movie pathetic ernest goes africa aside two three good laughs dispersed throughout film nothing positive hour half waste time life incredible someone able round group people willing act film edit piece trash even incredible eighth installment ernest series br br opening credits movie see ernest posing next various african objects wooden masks heads african animals making faces gestures would probably make 3rd graders laugh opening scene gives viewer taste ernest...   \n",
       "5208   film outstanding despite nc 17 rating disturbing scenes reality things like happen movie shows lot starts maya rosario dawson superb performance whose recently started attending college everything going well meets jared chad faust terrific performance frat party turns real gentleman sweet invites dinner look stars bridge end going apartment talk takes basement become flirtatious tries put end rapes incident scars goes club meets bartender dj adrian greatfully played marcus patrick sees getti...   \n",
       "26407                                                                                                                                                                                                  pleasure seeing saltimbanco live seeing video version show nothing compare actually people behind video amazing job capturing flavor feel sensation much wonderful performances saltimbanco stunningly amazing troupe beautifully captured throughout video flows smoothly artfully production wonderful experience   \n",
       "37814  think cheaters needs air end reality show care anyone says attack agree times like show spewing propaganda host cheaters joey greco little bastard wants think showing people camera effective unawares show disgusting also wiretapping following people cheaters spy illegal federal offense living police state like soviet union nazi germany rolled one happy poor reviews trash needs end soon going lose liberties nation wonder country going hell filth shows liked older shows better 1950s 1980s hope...   \n",
       "709    dolph lundgren back detention marks dolphs first film nearly 2 years following delayed hidden agenda film still marks improvement dolph cheapie trilogy jill rips agent red stormcatcher however film well standard hidden agenda better almost every respect film favour dolph previous outing sense cheesy fun film also rejuvenated dolph back high action role good see dolph stunts br br films story ludicrous prime b movie material ex military man teacher last day teaching whilst taking detention cl...   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            text_tokenized  \\\n",
       "48232  [honestly, know, begin, reviewing, movie, pathetic, ernest, goes, africa, aside, two, three, good, laughs, dispersed, throughout, film, nothing, positive, hour, half, waste, time, life, incredible, someone, able, round, group, people, willing, act, film, edit, piece, trash, even, incredible, eighth, installment, ernest, series, br, br, opening, credits, movie, see, ernest, posing, next, various, african, objects, wooden, masks, heads, african, animals, making, faces, gestures, would, probabl...   \n",
       "5208   [film, outstanding, despite, nc, 17, rating, disturbing, scenes, reality, things, like, happen, movie, shows, lot, starts, maya, rosario, dawson, superb, performance, whose, recently, started, attending, college, everything, going, well, meets, jared, chad, faust, terrific, performance, frat, party, turns, real, gentleman, sweet, invites, dinner, look, stars, bridge, end, going, apartment, talk, takes, basement, become, flirtatious, tries, put, end, rapes, incident, scars, goes, club, meets,...   \n",
       "26407                                                                                                                                                            [pleasure, seeing, saltimbanco, live, seeing, video, version, show, nothing, compare, actually, people, behind, video, amazing, job, capturing, flavor, feel, sensation, much, wonderful, performances, saltimbanco, stunningly, amazing, troupe, beautifully, captured, throughout, video, flows, smoothly, artfully, production, wonderful, experience]   \n",
       "37814  [think, cheaters, needs, air, end, reality, show, care, anyone, says, attack, agree, times, like, show, spewing, propaganda, host, cheaters, joey, greco, little, bastard, wants, think, showing, people, camera, effective, unawares, show, disgusting, also, wiretapping, following, people, cheaters, spy, illegal, federal, offense, living, police, state, like, soviet, union, nazi, germany, rolled, one, happy, poor, reviews, trash, needs, end, soon, going, lose, liberties, nation, wonder, country,...   \n",
       "709    [dolph, lundgren, back, detention, marks, dolphs, first, film, nearly, 2, years, following, delayed, hidden, agenda, film, still, marks, improvement, dolph, cheapie, trilogy, jill, rips, agent, red, stormcatcher, however, film, well, standard, hidden, agenda, better, almost, every, respect, film, favour, dolph, previous, outing, sense, cheesy, fun, film, also, rejuvenated, dolph, back, high, action, role, good, see, dolph, stunts, br, br, films, story, ludicrous, prime, b, movie, material, e...   \n",
       "\n",
       "       Pos  Neg  \n",
       "48232    0    1  \n",
       "5208     1    0  \n",
       "26407    1    0  \n",
       "37814    0    1  \n",
       "709      1    0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>text_tokenized</th>\n",
       "      <th>is_movie_related</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>44351</td>\n",
       "      <td>1</td>\n",
       "      <td>happy birthday niley supporter please watch vid</td>\n",
       "      <td>[happy, birthday, niley, supporter, please, watch, vid]</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1033662</td>\n",
       "      <td>0</td>\n",
       "      <td>netflix let watch instant play outside us either licensing</td>\n",
       "      <td>[netflix, let, watch, instant, play, outside, us, either, licensing]</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1563269</td>\n",
       "      <td>0</td>\n",
       "      <td>watching tv</td>\n",
       "      <td>[watching, tv]</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1511585</td>\n",
       "      <td>1</td>\n",
       "      <td>oh way home ready watch three hour raw special</td>\n",
       "      <td>[oh, way, home, ready, watch, three, hour, raw, special]</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1439166</td>\n",
       "      <td>0</td>\n",
       "      <td>forgot watch masterchef must watch catch tomorrow</td>\n",
       "      <td>[forgot, watch, masterchef, must, watch, catch, tomorrow]</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         label                                                        text  \\\n",
       "44351        1             happy birthday niley supporter please watch vid   \n",
       "1033662      0  netflix let watch instant play outside us either licensing   \n",
       "1563269      0                                                 watching tv   \n",
       "1511585      1              oh way home ready watch three hour raw special   \n",
       "1439166      0           forgot watch masterchef must watch catch tomorrow   \n",
       "\n",
       "                                                               text_tokenized  \\\n",
       "44351                 [happy, birthday, niley, supporter, please, watch, vid]   \n",
       "1033662  [netflix, let, watch, instant, play, outside, us, either, licensing]   \n",
       "1563269                                                        [watching, tv]   \n",
       "1511585              [oh, way, home, ready, watch, three, hour, raw, special]   \n",
       "1439166             [forgot, watch, masterchef, must, watch, catch, tomorrow]   \n",
       "\n",
       "         is_movie_related  Pos  Neg  \n",
       "44351                True    1    0  \n",
       "1033662              True    0    1  \n",
       "1563269              True    0    1  \n",
       "1511585              True    1    0  \n",
       "1439166              True    0    1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imdb_pre_processed = pd.read_pickle(\"imdb_pre_processed.pkl\")\n",
    "twitter_pre_processed = pd.read_pickle(\"twitter_pre_processed.pkl\")\n",
    "\n",
    "display(imdb_pre_processed.sample(5))\n",
    "display(twitter_pre_processed.sample(5))\n",
    "\n",
    "#Convert to numpy arrays\n",
    "sentences_imdb = imdb_pre_processed['text'].values\n",
    "y_imdb = imdb_pre_processed['label'].values\n",
    "\n",
    "sentences_twitter = twitter_pre_processed['text'].values\n",
    "y_twitter = twitter_pre_processed['label'].values\n",
    "\n",
    "#Generate a train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "sentences_train_imdb, sentences_test_imdb, y_train_imdb, y_test_imdb = train_test_split(sentences_imdb, \n",
    "                                                                                        y_imdb, test_size=0.25, random_state=47)\n",
    "\n",
    "sentences_train_twitter, sentences_test_twitter, y_train_twitter, y_test_twitter = train_test_split(sentences_twitter, \n",
    "                                                                                        y_twitter, test_size=0.25, random_state=47)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "vectorizer_imdb = CountVectorizer(max_features=10000)\n",
    "vectorizer_imdb.fit(sentences_train_imdb)\n",
    "X_train_imdb = vectorizer_imdb.transform(sentences_train_imdb)\n",
    "X_test_imdb  = vectorizer_imdb.transform(sentences_test_imdb)\n",
    "\n",
    "vectorizer_twitter = CountVectorizer(max_features=10000)\n",
    "vectorizer_twitter.fit(sentences_train_twitter)\n",
    "X_train_twitter = vectorizer_twitter.transform(sentences_train_twitter)\n",
    "X_test_twitter  = vectorizer_twitter.transform(sentences_test_twitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 2. We have decided that we are going to switch to a CNN here instead of a dense neural network. \n",
    "#The reason for this is that there are less weights to optimize, so we can evaluate deeper networks which will hopefully\n",
    "#Give us better performance. The embedding we chose is the google news word2vec pre-trained model. It includes word vectors for a\n",
    "#vocabulary of 3 million words and phrases that they trained on roughly 100 billion words from a google dataset. The vector length\n",
    "#is 300 features. We use gensim to load this word2vec embedding. \n",
    "imdb_pre_processed = pd.read_pickle(\"imdb_pre_processed.pkl\")\n",
    "twitter_pre_processed = pd.read_pickle(\"twitter_pre_processed.pkl\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "imdb_train, imdb_test = train_test_split(imdb_pre_processed, test_size=0.15, random_state=47)\n",
    "twitter_train, twitter_test = train_test_split(twitter_pre_processed, test_size=0.15, random_state=47)\n",
    "\n",
    "from gensim import models\n",
    "\n",
    "word2vec_path = 'GoogleNews-vectors-negative300.bin.gz'\n",
    "word2vec = models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5258596 words total, with a vocabulary size of 94536 (IMDB Train data)\n",
      "Max sentence length is 1455 (IMDB Train data)\n",
      "923208 words total, with a vocabulary size of 46003 (IMDB Test data)\n",
      "Max sentence length is 949 (IMDB Test data)\n",
      "406771 words total, with a vocabulary size of 32440 (Twitter Train data)\n",
      "Max sentence length is 27\n",
      "71707 words total, with a vocabulary size of 11597 (Twitter Test data)\n",
      "Max sentence length is 26 (Twitter Test data)\n"
     ]
    }
   ],
   "source": [
    "#Check length of words and vocabulary size so we can see if our word2vec model works correctly later. \n",
    "#IMDB \n",
    "all_training_words_imdb = [word for tokens in imdb_train[\"text_tokenized\"] for word in tokens]\n",
    "training_sentence_lengths_imdb = [len(tokens) for tokens in imdb_train[\"text_tokenized\"]]\n",
    "TRAINING_VOCAB_IMDB = sorted(list(set(all_training_words_imdb)))\n",
    "print(\"{} words total, with a vocabulary size of {} (IMDB Train data)\".format(len(all_training_words_imdb), len(TRAINING_VOCAB_IMDB)))\n",
    "print(\"Max sentence length is {} (IMDB Train data)\".format(max(training_sentence_lengths_imdb)))\n",
    "\n",
    "all_test_words_imdb = [word for tokens in imdb_test[\"text_tokenized\"] for word in tokens]\n",
    "test_sentence_lengths_imdb = [len(tokens) for tokens in imdb_test[\"text_tokenized\"]]\n",
    "TESTING_VOCAB_IMDB = sorted(list(set(all_test_words_imdb)))\n",
    "print(\"{} words total, with a vocabulary size of {} (IMDB Test data)\".format(len(all_test_words_imdb), len(TESTING_VOCAB_IMDB)))\n",
    "print(\"Max sentence length is {} (IMDB Test data)\".format(max(test_sentence_lengths_imdb)))\n",
    "\n",
    "\n",
    "#TWITTER\n",
    "all_training_words_twitter = [word for tokens in twitter_train[\"text_tokenized\"] for word in tokens]\n",
    "training_sentence_lengths_twitter = [len(tokens) for tokens in twitter_train[\"text_tokenized\"]]\n",
    "TRAINING_VOCAB_TWITTER = sorted(list(set(all_training_words_twitter)))\n",
    "print(\"{} words total, with a vocabulary size of {} (Twitter Train data)\".format(len(all_training_words_twitter), len(TRAINING_VOCAB_TWITTER)))\n",
    "print(\"Max sentence length is {}\".format(max(training_sentence_lengths_twitter)))\n",
    "\n",
    "all_test_words_twitter = [word for tokens in twitter_test[\"text_tokenized\"] for word in tokens]\n",
    "test_sentence_lengths_twitter = [len(tokens) for tokens in twitter_test[\"text_tokenized\"]]\n",
    "TESTING_VOCAB_twitter = sorted(list(set(all_test_words_twitter)))\n",
    "print(\"{} words total, with a vocabulary size of {} (Twitter Test data)\".format(len(all_test_words_twitter), len(TESTING_VOCAB_twitter)))\n",
    "print(\"Max sentence length is {} (Twitter Test data)\".format(max(test_sentence_lengths_twitter)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After the word2vec model, we use a function to generate the embeddings. The get_average_word2vec function\n",
    "#takes a list of tokenized words, vectors is the embedding we will use, and generate_missings is a boolean variable that will generate \n",
    "#a number for the missing words. If the length of a review is <1, we generate a np.array of zeros. \n",
    "def get_average_word2vec(tokenized_words, vector, generate_missing=False, k=300):\n",
    "    if len(tokenized_words)<1:\n",
    "        return np.zeros(k)\n",
    "    if generate_missing:\n",
    "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokenized_words]\n",
    "    else:\n",
    "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokenized_words]\n",
    "    length = len(vectorized)\n",
    "    summed = np.sum(vectorized, axis=0)\n",
    "    averaged = np.divide(summed, length)\n",
    "    return averaged\n",
    "\n",
    "#This function applies the above function to our data. \n",
    "def get_word2vec_embeddings(vectors, clean_comments, generate_missing=False):\n",
    "    embeddings = clean_comments['text_tokenized'].apply(lambda x: get_average_word2vec(x, vectors, \n",
    "                                                                                generate_missing))\n",
    "    return list(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_imdb_embeddings = get_word2vec_embeddings(word2vec, imdb_train, generate_missing=True)\n",
    "training_twitter_embeddings = get_word2vec_embeddings(word2vec, twitter_train, generate_missing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 94536 unique words.\n",
      "Found 32440 unique words.\n"
     ]
    }
   ],
   "source": [
    "#Tokenize the data, and pad the sequences so we have proper input into the CNN. \n",
    "MAX_SEQUENCE_LENGTH = 50\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "#Initialize the Tokenizer, with max words to keep as the numer of unique words in imdb dataset.\n",
    "tokenizer_imdb = Tokenizer(num_words=len(TRAINING_VOCAB_IMDB))\n",
    "tokenizer_twitter = Tokenizer(num_words=len(TRAINING_VOCAB_TWITTER))\n",
    "\n",
    "#Updates internal vocabulary based on a list of texts. This method creates the vocabulary index based on word frequency\n",
    "tokenizer_imdb.fit_on_texts(imdb_train[\"text\"].tolist())\n",
    "tokenizer_twitter.fit_on_texts(twitter_train[\"text\"].tolist())\n",
    "\n",
    "#Transforms each text in texts to a sequence of integers. \n",
    "#So it basically takes each word in the text and replaces it with its corresponding integer value from the word_index dictionary.\n",
    "training_sequences_imdb = tokenizer_imdb.texts_to_sequences(imdb_train[\"text\"].tolist())\n",
    "training_sequences_twitter = tokenizer_twitter.texts_to_sequences(twitter_train[\"text\"].tolist())\n",
    "\n",
    "#Word_index is a dictionary mapping words (str) to their rank/index (int). \n",
    "imdb_train_word_index = tokenizer_imdb.word_index \n",
    "print('Found {} unique words.'.format(len(imdb_train_word_index)))\n",
    "\n",
    "twitter_train_word_index = tokenizer_twitter.word_index\n",
    "print('Found {} unique words.'.format(len(twitter_train_word_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In the output from the question above, we can see that after the word embeddings the unique number of words is still the same!\n",
    "#So that part worked. \n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "train_cnn_imdb_data = pad_sequences(training_sequences_imdb, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "train_cnn_twitter_data = pad_sequences(training_sequences_twitter, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(94537, 300)\n",
      "(32441, 300)\n"
     ]
    }
   ],
   "source": [
    "#Initialize a np.zeros array with the dimensions of our training data. \n",
    "imdb_train_embedding_weights = np.zeros((len(imdb_train_word_index)+1, EMBEDDING_DIM))\n",
    "twitter_train_embedding_weights = np.zeros((len(twitter_train_word_index)+1, EMBEDDING_DIM))\n",
    "\n",
    "#Give the matrix the value of the word2vec embedding if the word is IN the word2vec embedding, \n",
    "#else give it a random number with the shape of EMBEDDING_DIM\n",
    "for word,index in imdb_train_word_index.items():\n",
    "    imdb_train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
    "    \n",
    "for word,index in twitter_train_word_index.items():\n",
    "    twitter_train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
    "    \n",
    "print(imdb_train_embedding_weights.shape)\n",
    "print(twitter_train_embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now training data is properly defined, of course we also need to define the testing data. \n",
    "test_sequences_imdb = tokenizer_imdb.texts_to_sequences(imdb_test[\"text\"].tolist())\n",
    "test_sequences_twitter = tokenizer_twitter.texts_to_sequences(twitter_test[\"text\"].tolist())\n",
    "\n",
    "test_cnn_imdb_data = pad_sequences(test_sequences_imdb, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_cnn_twitter_data = pad_sequences(test_sequences_twitter, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Conv1D, GlobalMaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, labels_index):\n",
    "    \n",
    "    #The embedding layer turns positive integers (indexes) into dense vectors of fixed size.\n",
    "    #It requires that the input data be integer encoded, so that each word is represented by a unique integer. \n",
    "    #Which is what we did previously. \n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            weights=[embeddings],\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=False)\n",
    "    \n",
    "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    convs = []\n",
    "    filter_sizes = [2,3,4,5,6]\n",
    "\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=200, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
    "        l_pool = GlobalMaxPooling1D()(l_conv)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "\n",
    "    l_merge = concatenate(convs, axis=1)\n",
    "\n",
    "    x = Dropout(0.1)(l_merge)  \n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    preds = Dense(labels_index, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make sure that y is in the right format. \n",
    "label_names = ['Pos', 'Neg']\n",
    "y_train_imdb = imdb_train[label_names].values\n",
    "y_train_twitter = twitter_train[label_names].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1126 17:45:49.529979  9856 deprecation_wrapper.py:119] From C:\\Users\\Yme\\Anaconda3\\envs\\DeepLearning3.6\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1126 17:45:49.590622  9856 deprecation_wrapper.py:119] From C:\\Users\\Yme\\Anaconda3\\envs\\DeepLearning3.6\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1126 17:45:49.601552  9856 deprecation_wrapper.py:119] From C:\\Users\\Yme\\Anaconda3\\envs\\DeepLearning3.6\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1126 17:45:49.626465  9856 deprecation_wrapper.py:119] From C:\\Users\\Yme\\Anaconda3\\envs\\DeepLearning3.6\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W1126 17:45:49.627461  9856 deprecation_wrapper.py:119] From C:\\Users\\Yme\\Anaconda3\\envs\\DeepLearning3.6\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W1126 17:45:55.411676  9856 deprecation.py:506] From C:\\Users\\Yme\\Anaconda3\\envs\\DeepLearning3.6\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W1126 17:45:55.494452  9856 deprecation_wrapper.py:119] From C:\\Users\\Yme\\Anaconda3\\envs\\DeepLearning3.6\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W1126 17:45:55.534346  9856 deprecation.py:323] From C:\\Users\\Yme\\Anaconda3\\envs\\DeepLearning3.6\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 50, 300)      28361100    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 49, 200)      120200      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 48, 200)      180200      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 47, 200)      240200      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 46, 200)      300200      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 45, 200)      360200      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 200)          0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 200)          0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 200)          0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 200)          0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 200)          0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1000)         0           global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "                                                                 global_max_pooling1d_4[0][0]     \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1000)         0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          128128      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2)            258         dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 29,690,486\n",
      "Trainable params: 1,329,386\n",
      "Non-trainable params: 28,361,100\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 50, 300)      9732300     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 49, 200)      120200      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 48, 200)      180200      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 47, 200)      240200      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 46, 200)      300200      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 45, 200)      360200      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 200)          0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_7 (GlobalM (None, 200)          0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_8 (GlobalM (None, 200)          0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_9 (GlobalM (None, 200)          0           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_10 (Global (None, 200)          0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 1000)         0           global_max_pooling1d_6[0][0]     \n",
      "                                                                 global_max_pooling1d_7[0][0]     \n",
      "                                                                 global_max_pooling1d_8[0][0]     \n",
      "                                                                 global_max_pooling1d_9[0][0]     \n",
      "                                                                 global_max_pooling1d_10[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 1000)         0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 128)          128128      dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 2)            258         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 11,061,686\n",
      "Trainable params: 1,329,386\n",
      "Non-trainable params: 9,732,300\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_imdb = ConvNet(imdb_train_embedding_weights, MAX_SEQUENCE_LENGTH, \n",
    "                len(imdb_train_word_index)+1, \n",
    "                EMBEDDING_DIM, len(list(label_names)))\n",
    "\n",
    "model_twitter = ConvNet(twitter_train_embedding_weights, MAX_SEQUENCE_LENGTH, \n",
    "                len(twitter_train_word_index)+1, \n",
    "                EMBEDDING_DIM, len(list(label_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 38250 samples, validate on 4250 samples\n",
      "Epoch 1/6\n",
      "38250/38250 [==============================] - 31s 806us/step - loss: 0.4477 - acc: 0.7845 - val_loss: 0.3587 - val_acc: 0.8395\n",
      "Epoch 2/6\n",
      "38250/38250 [==============================] - 27s 699us/step - loss: 0.3150 - acc: 0.8671 - val_loss: 0.3127 - val_acc: 0.8626\n",
      "Epoch 3/6\n",
      "38250/38250 [==============================] - 27s 702us/step - loss: 0.2087 - acc: 0.9189 - val_loss: 0.3323 - val_acc: 0.8693\n",
      "Epoch 4/6\n",
      "38250/38250 [==============================] - 27s 701us/step - loss: 0.1045 - acc: 0.9617 - val_loss: 0.4522 - val_acc: 0.8480\n",
      "Epoch 5/6\n",
      "38250/38250 [==============================] - 27s 698us/step - loss: 0.0596 - acc: 0.9778 - val_loss: 0.5095 - val_acc: 0.8468\n",
      "Epoch 6/6\n",
      "38250/38250 [==============================] - 27s 704us/step - loss: 0.0446 - acc: 0.9847 - val_loss: 0.6353 - val_acc: 0.8524\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 6\n",
    "batch_size = 34\n",
    "\n",
    "#Fit the imdb model\n",
    "final_imdb_model = model_imdb.fit(train_cnn_imdb_data, y_train_imdb, epochs=num_epochs, validation_split=0.1, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40770 samples, validate on 4531 samples\n",
      "Epoch 1/6\n",
      "40770/40770 [==============================] - 31s 760us/step - loss: 0.5388 - acc: 0.7294 - val_loss: 0.5130 - val_acc: 0.7544\n",
      "Epoch 2/6\n",
      "40770/40770 [==============================] - 29s 702us/step - loss: 0.4620 - acc: 0.7870 - val_loss: 0.4904 - val_acc: 0.7610\n",
      "Epoch 3/6\n",
      "40770/40770 [==============================] - 28s 697us/step - loss: 0.4023 - acc: 0.8194 - val_loss: 0.4931 - val_acc: 0.7669\n",
      "Epoch 4/6\n",
      "40770/40770 [==============================] - 29s 700us/step - loss: 0.3180 - acc: 0.8634 - val_loss: 0.5325 - val_acc: 0.7641\n",
      "Epoch 5/6\n",
      "40770/40770 [==============================] - 29s 704us/step - loss: 0.2307 - acc: 0.9058 - val_loss: 0.6263 - val_acc: 0.7625\n",
      "Epoch 6/6\n",
      "40770/40770 [==============================] - 29s 712us/step - loss: 0.1680 - acc: 0.9333 - val_loss: 0.7374 - val_acc: 0.7443\n"
     ]
    }
   ],
   "source": [
    "#Fit the twitter model. \n",
    "twitter_model = model_twitter.fit(train_cnn_twitter_data, y_train_twitter, epochs=num_epochs, validation_split=0.1, batch_size=batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
