{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Handle necessary imports\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "pd.set_option('max_colwidth', 500)\n",
    "pd.set_option('max_columns', 500)\n",
    "pd.set_option('max_rows', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 + Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, we load the IMDB movie dataset. Since all reviews are in seperate\n",
    "#.txt files, first we read them all in and add them to two lists. Then we create two dataframes \n",
    "#(1 for positive, 1 for negative),In which we have two columns: Reviews with review content, and sentiment with sentiment \n",
    "# value (1=positive, 0=negative).We repeat this process twice, once for the train data and one for the test data. \n",
    "# We save the results to csv files, so we don't ever have to re-do the process. \n",
    "# Then we create one big data-frame by appending the train dataframe to the test dataframe. \n",
    "pos_test_files = os.listdir(\"C:/Users/Yme/Documents/MEGA/Master DSE/Data Mining/Assignments/Assignment 3/data/aclImdb/test/pos\")\n",
    "neg_test_files = os.listdir(\"C:/Users/Yme/Documents/MEGA/Master DSE/Data Mining/Assignments/Assignment 3/data/aclImdb/test/neg\")\n",
    "pos_train_files = os.listdir(\"C:/Users/Yme/Documents/MEGA/Master DSE/Data Mining/Assignments/Assignment 3/data/aclImdb/train/pos\")\n",
    "neg_train_files = os.listdir(\"C:/Users/Yme/Documents/MEGA/Master DSE/Data Mining/Assignments/Assignment 3/data/aclImdb/train/neg\")\n",
    "\n",
    "train_pos_list = []\n",
    "for file in pos_train_files:\n",
    "    with open(\"C:/Users/Yme/Documents/MEGA/Master DSE/Data Mining/Assignments/Assignment 3/data/aclImdb/train/pos/{}\".format(file), 'r', encoding='utf-8') as fd:\n",
    "        text = fd.read()\n",
    "        train_pos_list.append(text)\n",
    "\n",
    "train_neg_list = []\n",
    "for file in neg_train_files:\n",
    "    with open(\"C:/Users/Yme/Documents/MEGA/Master DSE/Data Mining/Assignments/Assignment 3/data/aclImdb/train/neg/{}\".format(file), 'r',encoding='utf-8') as fd:\n",
    "        text = fd.read()\n",
    "        train_neg_list.append(text)\n",
    "\n",
    "train_data_pos = {'label' : 1, 'text': train_pos_list}\n",
    "train_data_neg = {'label' :0, 'text': train_neg_list}\n",
    "train_data_frame_complete = pd.DataFrame(train_data_pos).append(pd.DataFrame(train_data_neg)).reset_index(drop=True)\n",
    "train_data_frame_complete.to_csv('imdb_train.csv', index=False)\n",
    "\n",
    "test_pos_list = []\n",
    "for file in pos_test_files:\n",
    "    with open(\"C:/Users/Yme/Documents/MEGA/Master DSE/Data Mining/Assignments/Assignment 3/data/aclImdb/test/pos/{}\".format(file), 'r', encoding='utf-8') as fd:\n",
    "        text = fd.read()\n",
    "        test_pos_list.append(text)\n",
    "\n",
    "test_neg_list = []\n",
    "for file in neg_test_files:\n",
    "    with open(\"C:/Users/Yme/Documents/MEGA/Master DSE/Data Mining/Assignments/Assignment 3/data/aclImdb/test/neg/{}\".format(file), 'r',encoding='utf-8') as fd:\n",
    "        text = fd.read()\n",
    "        test_neg_list.append(text)\n",
    "\n",
    "test_data_pos = {'label' : 1, 'text': test_pos_list}\n",
    "test_data_neg = {'label' :0, 'text': test_neg_list}\n",
    "test_data_frame_complete = pd.DataFrame(test_data_pos).append(pd.DataFrame(test_data_neg)).reset_index(drop=True)\n",
    "test_data_frame_complete.to_csv('imdb_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>26247</td>\n",
       "      <td>1</td>\n",
       "      <td>Fame is one of the best movies I've seen about The Performing Arts. The music and the acting are excellent. The screenplay and Set Design are also excellent. My favorite part is when all the students start Dancing and making music in the Canteen. I can see this movie any number of times, and never get bored. I give it 8 1/2 on 10.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35067</td>\n",
       "      <td>1</td>\n",
       "      <td>This movie fully deserves to be one of the top Hindi comedies ever made . Rajkumar Santoshi is mostly known for his gritty hard-hitting social dramas , but this is easily the most effortless movie he has made .&lt;br /&gt;&lt;br /&gt;The plot revolves around two small-town buffoons Amar (Aamir Khan) and Prem (Salman Khan) . They want to get rich quick and so move to the big city . They hatch the same plan separately - to woo a rich heiress Raveena (Raveena Tandon) who is the daughter of a rich businessm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34590</td>\n",
       "      <td>1</td>\n",
       "      <td>in a time of predictable movies, in which abound violence, cheap romance and melodrama, it is delightfully surprising to find such a strange movie. the plot itself is compelling, and the actors are excellent, especially Alan Rickman. If you want to watch a movie that does not provide all the answers before asking the questions, a movie that will surprise you (in good or bad), Dark Harbor's for you. And if you're not convinced, believe me that Alan Rickman's performance is well worth it... es...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label  \\\n",
       "26247      1   \n",
       "35067      1   \n",
       "34590      1   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \n",
       "26247                                                                                                                                                                         Fame is one of the best movies I've seen about The Performing Arts. The music and the acting are excellent. The screenplay and Set Design are also excellent. My favorite part is when all the students start Dancing and making music in the Canteen. I can see this movie any number of times, and never get bored. I give it 8 1/2 on 10.  \n",
       "35067  This movie fully deserves to be one of the top Hindi comedies ever made . Rajkumar Santoshi is mostly known for his gritty hard-hitting social dramas , but this is easily the most effortless movie he has made .<br /><br />The plot revolves around two small-town buffoons Amar (Aamir Khan) and Prem (Salman Khan) . They want to get rich quick and so move to the big city . They hatch the same plan separately - to woo a rich heiress Raveena (Raveena Tandon) who is the daughter of a rich businessm...  \n",
       "34590  in a time of predictable movies, in which abound violence, cheap romance and melodrama, it is delightfully surprising to find such a strange movie. the plot itself is compelling, and the actors are excellent, especially Alan Rickman. If you want to watch a movie that does not provide all the answers before asking the questions, a movie that will surprise you (in good or bad), Dark Harbor's for you. And if you're not convinced, believe me that Alan Rickman's performance is well worth it... es...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Then we load all data, and create one big csv file. This csv file has the first 25k rows as \"train data\", and \n",
    "#the last 25k rows as \"test data\". Of course, this division is arbitrary (It was the way the dataset was divided when downloaded),\n",
    "#and we can specify our own train/test ratio later on. \n",
    "# df_imdb_train = pd.read_csv(\"imdb_train.csv\")\n",
    "# df_imdb_test = pd.read_csv(\"imdb_test.csv\")\n",
    "# imdb_final = df_imdb_train.append(df_imdb_test, ignore_index=True)\n",
    "# imdb_final.to_csv(\"imdb_final.csv\", index=False)\n",
    "\n",
    "# Print some information to check if all went well. We still have to do some pre-processing.\n",
    "#We see that there is still some HTML tags in the data. Also,there is some punctuation, and special characters.\n",
    "#Additional pre-processing will be needed. \n",
    "imdb_final = pd.read_csv(\"imdb_final.csv\")\n",
    "display(imdb_final.sample(3, random_state=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>734670</td>\n",
       "      <td>0</td>\n",
       "      <td>Got three shots today.   I can't lift my arms...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139717</td>\n",
       "      <td>0</td>\n",
       "      <td>@ebassman Rock it tonight in DC. Wish I was there!  Saw @nkotb in DC last October, but will have to settle for Denver this year!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1339568</td>\n",
       "      <td>1</td>\n",
       "      <td>Who's going out tonight? I want to go out  xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1153750</td>\n",
       "      <td>1</td>\n",
       "      <td>Photo: Let me know if you wanna buy knives that can cut through pennies. Iï¿½m a cutco rep.  http://tumblr.com/xwm1wa922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>259541</td>\n",
       "      <td>1</td>\n",
       "      <td>@kimbannerman First he needs to read your book!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825742</td>\n",
       "      <td>0</td>\n",
       "      <td>I have a yucky ucky headache. AND a sore throat.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         label  \\\n",
       "734670       0   \n",
       "139717       0   \n",
       "1339568      1   \n",
       "1153750      1   \n",
       "259541       1   \n",
       "825742       0   \n",
       "\n",
       "                                                                                                                                     text  \n",
       "734670                                                                                   Got three shots today.   I can't lift my arms...  \n",
       "139717   @ebassman Rock it tonight in DC. Wish I was there!  Saw @nkotb in DC last October, but will have to settle for Denver this year!  \n",
       "1339568                                                                                    Who's going out tonight? I want to go out  xxx  \n",
       "1153750          Photo: Let me know if you wanna buy knives that can cut through pennies. Iï¿½m a cutco rep.  http://tumblr.com/xwm1wa922  \n",
       "259541                                                                                   @kimbannerman First he needs to read your book!   \n",
       "825742                                                                                  I have a yucky ucky headache. AND a sore throat.   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Then, we load in Twitter data. The dataset we used is a \"dumbed down/light version\" of the Sentiment140 dataset that was \n",
    "#provided in the link you gave us. http://help.sentiment140.com/ is the source of the original dataset; and it can be found \n",
    "#https://www.kaggle.com/kazanova/sentiment140 here in its full form. However, we stumbled upon a more suitable version of the dataset\n",
    "#here: https://www.kaggle.com/ywang311/twitter-sentiment. This dataset doesn't have all the columns we do not need.\n",
    "#Since we want to compare the IMDB movie ratings sentiments with the twitter sentiment, we filter all tweets about movies.  \n",
    "twitter = pd.read_csv(\"C:/Users/Yme/Documents/MEGA/Master DSE/Data Mining/Assignments/Assignment 3/data/Sentiment Analysis Dataset 2.csv\", skiprows=[8835,535881] , usecols = ['Sentiment' , 'SentimentText'])\n",
    "twitter = twitter.rename(columns = {'Sentiment': 'label' , 'SentimentText':'text'})\n",
    "display(twitter.sample(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>26247</td>\n",
       "      <td>1</td>\n",
       "      <td>fame one best movies seen performing arts music acting excellent screenplay set design also excellent favorite part students start dancing making music canteen see movie number times never get bored give 8 1 2 10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35067</td>\n",
       "      <td>1</td>\n",
       "      <td>movie fully deserves one top hindi comedies ever made rajkumar santoshi mostly known gritty hard hitting social dramas easily effortless movie made br br plot revolves around two small town buffoons amar aamir khan prem salman khan want get rich quick move big city hatch plan separately woo rich heiress raveena raveena tandon daughter rich businessman ramgopal bajaj paresh rawal thus one marries raveena gets hands wealth get know plan intense tussle one oneupmanship marries raveena hilarious...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34590</td>\n",
       "      <td>1</td>\n",
       "      <td>time predictable movies abound violence cheap romance melodrama delightfully surprising find strange movie plot compelling actors excellent especially alan rickman want watch movie provide answers asking questions movie surprise good bad dark harbor convinced believe alan rickman performance well worth especially end ladies</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label  \\\n",
       "26247      1   \n",
       "35067      1   \n",
       "34590      1   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \n",
       "26247                                                                                                                                                                                                                                                                                                 fame one best movies seen performing arts music acting excellent screenplay set design also excellent favorite part students start dancing making music canteen see movie number times never get bored give 8 1 2 10  \n",
       "35067  movie fully deserves one top hindi comedies ever made rajkumar santoshi mostly known gritty hard hitting social dramas easily effortless movie made br br plot revolves around two small town buffoons amar aamir khan prem salman khan want get rich quick move big city hatch plan separately woo rich heiress raveena raveena tandon daughter rich businessman ramgopal bajaj paresh rawal thus one marries raveena gets hands wealth get know plan intense tussle one oneupmanship marries raveena hilarious...  \n",
       "34590                                                                                                                                                                                time predictable movies abound violence cheap romance melodrama delightfully surprising find strange movie plot compelling actors excellent especially alan rickman want watch movie provide answers asking questions movie surprise good bad dark harbor convinced believe alan rickman performance well worth especially end ladies  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1564943</td>\n",
       "      <td>1</td>\n",
       "      <td>well time bed 5 00 comes early nice chatting everyone good evening rest weekend left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1287022</td>\n",
       "      <td>1</td>\n",
       "      <td>defyingsantafe umm forget gay socialist atheists</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1526836</td>\n",
       "      <td>0</td>\n",
       "      <td>mom nearly got ran truck bike dropped work bag information stolen fb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         label  \\\n",
       "1564943      1   \n",
       "1287022      1   \n",
       "1526836      0   \n",
       "\n",
       "                                                                                         text  \n",
       "1564943  well time bed 5 00 comes early nice chatting everyone good evening rest weekend left  \n",
       "1287022                                      defyingsantafe umm forget gay socialist atheists  \n",
       "1526836                  mom nearly got ran truck bike dropped work bag information stolen fb  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Remove link, user, special characters and stopwords.\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "#Please note that this somewhat complex regex was found online... First we did\n",
    "# a lot of the pre-processing manually, but then we stumbled upon this regex \n",
    "#And are very happy that smart people took care of building this regex for us =)\n",
    "TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
    "\n",
    "def preprocess(text):\n",
    "    text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n",
    "    tokens = []\n",
    "    for token in text.split():\n",
    "        if token not in stop_words:\n",
    "                tokens.append(token)\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "imdb_final['text'] = imdb_final['text'].apply(lambda x: preprocess(x))\n",
    "display(imdb_final.sample(3, random_state=1))\n",
    "\n",
    "twitter['text'] = twitter['text'].apply(lambda x: preprocess(x))\n",
    "display(twitter.sample(3, random_state=1))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>text_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1564943</td>\n",
       "      <td>1</td>\n",
       "      <td>well time bed 5 00 comes early nice chatting everyone good evening rest weekend left</td>\n",
       "      <td>[well, time, bed, 5, 00, comes, early, nice, chatting, everyone, good, evening, rest, weekend, left]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1287022</td>\n",
       "      <td>1</td>\n",
       "      <td>defyingsantafe umm forget gay socialist atheists</td>\n",
       "      <td>[defyingsantafe, umm, forget, gay, socialist, atheists]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1526836</td>\n",
       "      <td>0</td>\n",
       "      <td>mom nearly got ran truck bike dropped work bag information stolen fb</td>\n",
       "      <td>[mom, nearly, got, ran, truck, bike, dropped, work, bag, information, stolen, fb]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         label  \\\n",
       "1564943      1   \n",
       "1287022      1   \n",
       "1526836      0   \n",
       "\n",
       "                                                                                         text  \\\n",
       "1564943  well time bed 5 00 comes early nice chatting everyone good evening rest weekend left   \n",
       "1287022                                      defyingsantafe umm forget gay socialist atheists   \n",
       "1526836                  mom nearly got ran truck bike dropped work bag information stolen fb   \n",
       "\n",
       "                                                                                               text_tokenized  \n",
       "1564943  [well, time, bed, 5, 00, comes, early, nice, chatting, everyone, good, evening, rest, weekend, left]  \n",
       "1287022                                               [defyingsantafe, umm, forget, gay, socialist, atheists]  \n",
       "1526836                     [mom, nearly, got, ran, truck, bike, dropped, work, bag, information, stolen, fb]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>text_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>26247</td>\n",
       "      <td>1</td>\n",
       "      <td>fame one best movies seen performing arts music acting excellent screenplay set design also excellent favorite part students start dancing making music canteen see movie number times never get bored give 8 1 2 10</td>\n",
       "      <td>[fame, one, best, movies, seen, performing, arts, music, acting, excellent, screenplay, set, design, also, excellent, favorite, part, students, start, dancing, making, music, canteen, see, movie, number, times, never, get, bored, give, 8, 1, 2, 10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35067</td>\n",
       "      <td>1</td>\n",
       "      <td>movie fully deserves one top hindi comedies ever made rajkumar santoshi mostly known gritty hard hitting social dramas easily effortless movie made br br plot revolves around two small town buffoons amar aamir khan prem salman khan want get rich quick move big city hatch plan separately woo rich heiress raveena raveena tandon daughter rich businessman ramgopal bajaj paresh rawal thus one marries raveena gets hands wealth get know plan intense tussle one oneupmanship marries raveena hilarious...</td>\n",
       "      <td>[movie, fully, deserves, one, top, hindi, comedies, ever, made, rajkumar, santoshi, mostly, known, gritty, hard, hitting, social, dramas, easily, effortless, movie, made, br, br, plot, revolves, around, two, small, town, buffoons, amar, aamir, khan, prem, salman, khan, want, get, rich, quick, move, big, city, hatch, plan, separately, woo, rich, heiress, raveena, raveena, tandon, daughter, rich, businessman, ramgopal, bajaj, paresh, rawal, thus, one, marries, raveena, gets, hands, wealth, get...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34590</td>\n",
       "      <td>1</td>\n",
       "      <td>time predictable movies abound violence cheap romance melodrama delightfully surprising find strange movie plot compelling actors excellent especially alan rickman want watch movie provide answers asking questions movie surprise good bad dark harbor convinced believe alan rickman performance well worth especially end ladies</td>\n",
       "      <td>[time, predictable, movies, abound, violence, cheap, romance, melodrama, delightfully, surprising, find, strange, movie, plot, compelling, actors, excellent, especially, alan, rickman, want, watch, movie, provide, answers, asking, questions, movie, surprise, good, bad, dark, harbor, convinced, believe, alan, rickman, performance, well, worth, especially, end, ladies]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label  \\\n",
       "26247      1   \n",
       "35067      1   \n",
       "34590      1   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
       "26247                                                                                                                                                                                                                                                                                                 fame one best movies seen performing arts music acting excellent screenplay set design also excellent favorite part students start dancing making music canteen see movie number times never get bored give 8 1 2 10   \n",
       "35067  movie fully deserves one top hindi comedies ever made rajkumar santoshi mostly known gritty hard hitting social dramas easily effortless movie made br br plot revolves around two small town buffoons amar aamir khan prem salman khan want get rich quick move big city hatch plan separately woo rich heiress raveena raveena tandon daughter rich businessman ramgopal bajaj paresh rawal thus one marries raveena gets hands wealth get know plan intense tussle one oneupmanship marries raveena hilarious...   \n",
       "34590                                                                                                                                                                                time predictable movies abound violence cheap romance melodrama delightfully surprising find strange movie plot compelling actors excellent especially alan rickman want watch movie provide answers asking questions movie surprise good bad dark harbor convinced believe alan rickman performance well worth especially end ladies   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            text_tokenized  \n",
       "26247                                                                                                                                                                                                                                                             [fame, one, best, movies, seen, performing, arts, music, acting, excellent, screenplay, set, design, also, excellent, favorite, part, students, start, dancing, making, music, canteen, see, movie, number, times, never, get, bored, give, 8, 1, 2, 10]  \n",
       "35067  [movie, fully, deserves, one, top, hindi, comedies, ever, made, rajkumar, santoshi, mostly, known, gritty, hard, hitting, social, dramas, easily, effortless, movie, made, br, br, plot, revolves, around, two, small, town, buffoons, amar, aamir, khan, prem, salman, khan, want, get, rich, quick, move, big, city, hatch, plan, separately, woo, rich, heiress, raveena, raveena, tandon, daughter, rich, businessman, ramgopal, bajaj, paresh, rawal, thus, one, marries, raveena, gets, hands, wealth, get...  \n",
       "34590                                                                                                                                    [time, predictable, movies, abound, violence, cheap, romance, melodrama, delightfully, surprising, find, strange, movie, plot, compelling, actors, excellent, especially, alan, rickman, want, watch, movie, provide, answers, asking, questions, movie, surprise, good, bad, dark, harbor, convinced, believe, alan, rickman, performance, well, worth, especially, end, ladies]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Tokenize the words (This breaks up the strings into a list of words or pieces)\n",
    "twitter['text_tokenized'] = [_text.split() for _text in twitter['text']] \n",
    "imdb_final['text_tokenized'] = [_text.split() for _text in imdb_final['text']] \n",
    "display(twitter.sample(3, random_state=1)) \n",
    "display(imdb_final.sample(3, random_state=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>text_tokenized</th>\n",
       "      <th>is_movie_related</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>871270</td>\n",
       "      <td>0</td>\n",
       "      <td>want watch dora movieeeeee</td>\n",
       "      <td>[want, watch, dora, movieeeeee]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1185304</td>\n",
       "      <td>1</td>\n",
       "      <td>saw speidi today lots amazing car including aston martin new bond movie</td>\n",
       "      <td>[saw, speidi, today, lots, amazing, car, including, aston, martin, new, bond, movie]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1562816</td>\n",
       "      <td>0</td>\n",
       "      <td>watching da vinci code alone appartment</td>\n",
       "      <td>[watching, da, vinci, code, alone, appartment]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91151</td>\n",
       "      <td>1</td>\n",
       "      <td>hey 2 u 2 wanna watch awards lol</td>\n",
       "      <td>[hey, 2, u, 2, wanna, watch, awards, lol]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1057186</td>\n",
       "      <td>1</td>\n",
       "      <td>nvm earphones watched wizards waverly place online lol felt like watching thanks internet</td>\n",
       "      <td>[nvm, earphones, watched, wizards, waverly, place, online, lol, felt, like, watching, thanks, internet]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4058</td>\n",
       "      <td>1</td>\n",
       "      <td>wonderful way waste day watching harold kumar movies</td>\n",
       "      <td>[wonderful, way, waste, day, watching, harold, kumar, movies]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         label  \\\n",
       "871270       0   \n",
       "1185304      1   \n",
       "1562816      0   \n",
       "91151        1   \n",
       "1057186      1   \n",
       "4058         1   \n",
       "\n",
       "                                                                                              text  \\\n",
       "871270                                                                  want watch dora movieeeeee   \n",
       "1185304                    saw speidi today lots amazing car including aston martin new bond movie   \n",
       "1562816                                                    watching da vinci code alone appartment   \n",
       "91151                                                             hey 2 u 2 wanna watch awards lol   \n",
       "1057186  nvm earphones watched wizards waverly place online lol felt like watching thanks internet   \n",
       "4058                                          wonderful way waste day watching harold kumar movies   \n",
       "\n",
       "                                                                                                  text_tokenized  \\\n",
       "871270                                                                           [want, watch, dora, movieeeeee]   \n",
       "1185304                     [saw, speidi, today, lots, amazing, car, including, aston, martin, new, bond, movie]   \n",
       "1562816                                                           [watching, da, vinci, code, alone, appartment]   \n",
       "91151                                                                  [hey, 2, u, 2, wanna, watch, awards, lol]   \n",
       "1057186  [nvm, earphones, watched, wizards, waverly, place, online, lol, felt, like, watching, thanks, internet]   \n",
       "4058                                               [wonderful, way, waste, day, watching, harold, kumar, movies]   \n",
       "\n",
       "         is_movie_related  \n",
       "871270               True  \n",
       "1185304              True  \n",
       "1562816              True  \n",
       "91151                True  \n",
       "1057186              True  \n",
       "4058                 True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Since our goal is to compare tweets about movies and imdb movie reviews, we filter the twitter dataset\n",
    "#on movie-related words. Please note that this list is completely arbitrary, and it is an assumption\n",
    "#that all the tweets which have these words in them are actually about movies.\n",
    "MOVIE_RELATED_WORDS = set([\"movie\", \"movies\", \"watch\", \n",
    "                           \"watching\", \"film\", \"cinema\", \n",
    "                           \"actor\", \"thriller\", \n",
    "                           \"horror\", \"dvd\", \"bluray\", \"soundtrack\", \n",
    "                            \"director\", \"remake\", \"blockbuster\"])\n",
    "\n",
    "def contains_movie_word(words):\n",
    "    return any(word in MOVIE_RELATED_WORDS for word in words)\n",
    "\n",
    "twitter['is_movie_related'] = twitter['text_tokenized'].apply(contains_movie_word)\n",
    "twitter_movie = twitter[twitter['is_movie_related']==True]\n",
    "display(twitter_movie.sample(6, random_state=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add positve and negative labels to the dataframe, for later use in the CNN\n",
    "pos_imdb = []\n",
    "neg_imdb = []\n",
    "for l in imdb_final['label']:\n",
    "    if l == 0:\n",
    "        pos_imdb.append(0)\n",
    "        neg_imdb.append(1)\n",
    "    elif l == 1:\n",
    "        pos_imdb.append(1)\n",
    "        neg_imdb.append(0)\n",
    "\n",
    "pos_tweet = []\n",
    "neg_tweet = []\n",
    "for l in twitter_movie['label']:\n",
    "    if l == 0:\n",
    "        pos_tweet.append(0)\n",
    "        neg_tweet.append(1)\n",
    "    elif l == 1:\n",
    "        pos_tweet.append(1)\n",
    "        neg_tweet.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yme\\Anaconda3\\envs\\DeepLearning3.6\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\Yme\\Anaconda3\\envs\\DeepLearning3.6\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>text_tokenized</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>26247</td>\n",
       "      <td>1</td>\n",
       "      <td>fame one best movies seen performing arts music acting excellent screenplay set design also excellent favorite part students start dancing making music canteen see movie number times never get bored give 8 1 2 10</td>\n",
       "      <td>[fame, one, best, movies, seen, performing, arts, music, acting, excellent, screenplay, set, design, also, excellent, favorite, part, students, start, dancing, making, music, canteen, see, movie, number, times, never, get, bored, give, 8, 1, 2, 10]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35067</td>\n",
       "      <td>1</td>\n",
       "      <td>movie fully deserves one top hindi comedies ever made rajkumar santoshi mostly known gritty hard hitting social dramas easily effortless movie made br br plot revolves around two small town buffoons amar aamir khan prem salman khan want get rich quick move big city hatch plan separately woo rich heiress raveena raveena tandon daughter rich businessman ramgopal bajaj paresh rawal thus one marries raveena gets hands wealth get know plan intense tussle one oneupmanship marries raveena hilarious...</td>\n",
       "      <td>[movie, fully, deserves, one, top, hindi, comedies, ever, made, rajkumar, santoshi, mostly, known, gritty, hard, hitting, social, dramas, easily, effortless, movie, made, br, br, plot, revolves, around, two, small, town, buffoons, amar, aamir, khan, prem, salman, khan, want, get, rich, quick, move, big, city, hatch, plan, separately, woo, rich, heiress, raveena, raveena, tandon, daughter, rich, businessman, ramgopal, bajaj, paresh, rawal, thus, one, marries, raveena, gets, hands, wealth, get...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34590</td>\n",
       "      <td>1</td>\n",
       "      <td>time predictable movies abound violence cheap romance melodrama delightfully surprising find strange movie plot compelling actors excellent especially alan rickman want watch movie provide answers asking questions movie surprise good bad dark harbor convinced believe alan rickman performance well worth especially end ladies</td>\n",
       "      <td>[time, predictable, movies, abound, violence, cheap, romance, melodrama, delightfully, surprising, find, strange, movie, plot, compelling, actors, excellent, especially, alan, rickman, want, watch, movie, provide, answers, asking, questions, movie, surprise, good, bad, dark, harbor, convinced, believe, alan, rickman, performance, well, worth, especially, end, ladies]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label  \\\n",
       "26247      1   \n",
       "35067      1   \n",
       "34590      1   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
       "26247                                                                                                                                                                                                                                                                                                 fame one best movies seen performing arts music acting excellent screenplay set design also excellent favorite part students start dancing making music canteen see movie number times never get bored give 8 1 2 10   \n",
       "35067  movie fully deserves one top hindi comedies ever made rajkumar santoshi mostly known gritty hard hitting social dramas easily effortless movie made br br plot revolves around two small town buffoons amar aamir khan prem salman khan want get rich quick move big city hatch plan separately woo rich heiress raveena raveena tandon daughter rich businessman ramgopal bajaj paresh rawal thus one marries raveena gets hands wealth get know plan intense tussle one oneupmanship marries raveena hilarious...   \n",
       "34590                                                                                                                                                                                time predictable movies abound violence cheap romance melodrama delightfully surprising find strange movie plot compelling actors excellent especially alan rickman want watch movie provide answers asking questions movie surprise good bad dark harbor convinced believe alan rickman performance well worth especially end ladies   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            text_tokenized  \\\n",
       "26247                                                                                                                                                                                                                                                             [fame, one, best, movies, seen, performing, arts, music, acting, excellent, screenplay, set, design, also, excellent, favorite, part, students, start, dancing, making, music, canteen, see, movie, number, times, never, get, bored, give, 8, 1, 2, 10]   \n",
       "35067  [movie, fully, deserves, one, top, hindi, comedies, ever, made, rajkumar, santoshi, mostly, known, gritty, hard, hitting, social, dramas, easily, effortless, movie, made, br, br, plot, revolves, around, two, small, town, buffoons, amar, aamir, khan, prem, salman, khan, want, get, rich, quick, move, big, city, hatch, plan, separately, woo, rich, heiress, raveena, raveena, tandon, daughter, rich, businessman, ramgopal, bajaj, paresh, rawal, thus, one, marries, raveena, gets, hands, wealth, get...   \n",
       "34590                                                                                                                                    [time, predictable, movies, abound, violence, cheap, romance, melodrama, delightfully, surprising, find, strange, movie, plot, compelling, actors, excellent, especially, alan, rickman, want, watch, movie, provide, answers, asking, questions, movie, surprise, good, bad, dark, harbor, convinced, believe, alan, rickman, performance, well, worth, especially, end, ladies]   \n",
       "\n",
       "       Pos  Neg  \n",
       "26247    1    0  \n",
       "35067    1    0  \n",
       "34590    1    0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>text_tokenized</th>\n",
       "      <th>is_movie_related</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>871270</td>\n",
       "      <td>0</td>\n",
       "      <td>want watch dora movieeeeee</td>\n",
       "      <td>[want, watch, dora, movieeeeee]</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1185304</td>\n",
       "      <td>1</td>\n",
       "      <td>saw speidi today lots amazing car including aston martin new bond movie</td>\n",
       "      <td>[saw, speidi, today, lots, amazing, car, including, aston, martin, new, bond, movie]</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1562816</td>\n",
       "      <td>0</td>\n",
       "      <td>watching da vinci code alone appartment</td>\n",
       "      <td>[watching, da, vinci, code, alone, appartment]</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         label  \\\n",
       "871270       0   \n",
       "1185304      1   \n",
       "1562816      0   \n",
       "\n",
       "                                                                            text  \\\n",
       "871270                                                want watch dora movieeeeee   \n",
       "1185304  saw speidi today lots amazing car including aston martin new bond movie   \n",
       "1562816                                  watching da vinci code alone appartment   \n",
       "\n",
       "                                                                               text_tokenized  \\\n",
       "871270                                                        [want, watch, dora, movieeeeee]   \n",
       "1185304  [saw, speidi, today, lots, amazing, car, including, aston, martin, new, bond, movie]   \n",
       "1562816                                        [watching, da, vinci, code, alone, appartment]   \n",
       "\n",
       "         is_movie_related  Pos  Neg  \n",
       "871270               True    0    1  \n",
       "1185304              True    1    0  \n",
       "1562816              True    0    1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imdb_final['Pos']= pos_imdb\n",
    "imdb_final['Neg']= neg_imdb\n",
    "\n",
    "twitter_movie['Pos']= pos_tweet\n",
    "twitter_movie['Neg']= neg_tweet\n",
    "\n",
    "display(imdb_final.sample(3, random_state=1))\n",
    "display(twitter_movie.sample(3, random_state=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For a basic representation to train our first neural network, we use a  CountVectorizer. \n",
    "#This is one of the most \"simple\" text embeddings. In Question 2 we will try another (more suitable) one.\n",
    "#This provides a simple way to tokenize a collection of text  documents and build a vocabulary of known words. \n",
    "#We chose this approach instead of the one-hot encoding you suggested to us. The reason for this is you referred us to\n",
    "#lecture 8, but lecture 8 uses the built in get_word_index() function from the imdb dataset, \n",
    "#which we cannot use for present data. \n",
    "\n",
    "# #Write files to pickle for later use\n",
    "twitter_movie.to_pickle(\"twitter_pre_processed.pkl\")\n",
    "imdb_final.to_pickle(\"imdb_pre_processed.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>text_tokenized</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>41631</td>\n",
       "      <td>0</td>\n",
       "      <td>2 deathly unfunny girls stays deathly unfunny uncle benny beach house uncle beeny like party guess deathly unfunny girls yup guessed deathly unfunny beach party catch subliminal message trying convey first moron would rather watch nude jello tag team watching match bea aurther cameryn manhiem vs rosie donnell jessica tandy movie lose term loosely bad br br grade f br br eye candy kristin novak charity rahmer go topless iva singer shows breasts buns</td>\n",
       "      <td>[2, deathly, unfunny, girls, stays, deathly, unfunny, uncle, benny, beach, house, uncle, beeny, like, party, guess, deathly, unfunny, girls, yup, guessed, deathly, unfunny, beach, party, catch, subliminal, message, trying, convey, first, moron, would, rather, watch, nude, jello, tag, team, watching, match, bea, aurther, cameryn, manhiem, vs, rosie, donnell, jessica, tandy, movie, lose, term, loosely, bad, br, br, grade, f, br, br, eye, candy, kristin, novak, charity, rahmer, go, topless, iva...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4372</td>\n",
       "      <td>1</td>\n",
       "      <td>la teta la luna symbolic spain film everything film occurs symbolic meaning totally different usual movie one access br br film good good people want look meaning everything film tale must advice sample film br br please enjoy</td>\n",
       "      <td>[la, teta, la, luna, symbolic, spain, film, everything, film, occurs, symbolic, meaning, totally, different, usual, movie, one, access, br, br, film, good, good, people, want, look, meaning, everything, film, tale, must, advice, sample, film, br, br, please, enjoy]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3425</td>\n",
       "      <td>1</td>\n",
       "      <td>basic storyline aditiya kumar spoilt son millionaire ishwar bachan owns toy industry ishwar eyes son aditya nothing wrong aditya mother sumitra shefali shah warns ishwar bring son responsible path late ishwar patient lung cancer 9 months live son elopes marries mitali chopra ishwar readily forgives aditya happy couple aditya mitali come back honeymoon mitali pregnant forces ishwar kick aditya house make responsible aditya know father suffering lung cancer also know father kicked hose make re...</td>\n",
       "      <td>[basic, storyline, aditiya, kumar, spoilt, son, millionaire, ishwar, bachan, owns, toy, industry, ishwar, eyes, son, aditya, nothing, wrong, aditya, mother, sumitra, shefali, shah, warns, ishwar, bring, son, responsible, path, late, ishwar, patient, lung, cancer, 9, months, live, son, elopes, marries, mitali, chopra, ishwar, readily, forgives, aditya, happy, couple, aditya, mitali, come, back, honeymoon, mitali, pregnant, forces, ishwar, kick, aditya, house, make, responsible, aditya, know, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44256</td>\n",
       "      <td>0</td>\n",
       "      <td>wait till watch one mean even reading review movie till date sucked one one thing wont understand ripping english flick add creativity amount spent making movie producers considered buying rights cellular dubbed hindi released movie might gotten profits way guess chance rate movie 0 would done pathetic performances come tanushree datta girl played sidekick aftab know problem seen cellular much earlier movie cant reason support movie could go hours neither time discuss useless crap movie want...</td>\n",
       "      <td>[wait, till, watch, one, mean, even, reading, review, movie, till, date, sucked, one, one, thing, wont, understand, ripping, english, flick, add, creativity, amount, spent, making, movie, producers, considered, buying, rights, cellular, dubbed, hindi, released, movie, might, gotten, profits, way, guess, chance, rate, movie, 0, would, done, pathetic, performances, come, tanushree, datta, girl, played, sidekick, aftab, know, problem, seen, cellular, much, earlier, movie, cant, reason, support,...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18958</td>\n",
       "      <td>0</td>\n",
       "      <td>movie possibly cheapest cheesiest poorest sequel ever made br br yet funniest idiotic movie disney guarantee laughs sappy stories lame plots start finish br br group short stories seem like bad fanfictions br br spoiler alert first one beast belle petty pathetic argument three loser new characters decide patch things forging letter forgiveness give belle part way little episode belle wall eyes made siblings laugh hard beast fight letter later learn meaning forgiveness old certainly old enoug...</td>\n",
       "      <td>[movie, possibly, cheapest, cheesiest, poorest, sequel, ever, made, br, br, yet, funniest, idiotic, movie, disney, guarantee, laughs, sappy, stories, lame, plots, start, finish, br, br, group, short, stories, seem, like, bad, fanfictions, br, br, spoiler, alert, first, one, beast, belle, petty, pathetic, argument, three, loser, new, characters, decide, patch, things, forging, letter, forgiveness, give, belle, part, way, little, episode, belle, wall, eyes, made, siblings, laugh, hard, beast, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label  \\\n",
       "41631      0   \n",
       "4372       1   \n",
       "3425       1   \n",
       "44256      0   \n",
       "18958      0   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
       "41631                                                 2 deathly unfunny girls stays deathly unfunny uncle benny beach house uncle beeny like party guess deathly unfunny girls yup guessed deathly unfunny beach party catch subliminal message trying convey first moron would rather watch nude jello tag team watching match bea aurther cameryn manhiem vs rosie donnell jessica tandy movie lose term loosely bad br br grade f br br eye candy kristin novak charity rahmer go topless iva singer shows breasts buns   \n",
       "4372                                                                                                                                                                                                                                                                                    la teta la luna symbolic spain film everything film occurs symbolic meaning totally different usual movie one access br br film good good people want look meaning everything film tale must advice sample film br br please enjoy   \n",
       "3425   basic storyline aditiya kumar spoilt son millionaire ishwar bachan owns toy industry ishwar eyes son aditya nothing wrong aditya mother sumitra shefali shah warns ishwar bring son responsible path late ishwar patient lung cancer 9 months live son elopes marries mitali chopra ishwar readily forgives aditya happy couple aditya mitali come back honeymoon mitali pregnant forces ishwar kick aditya house make responsible aditya know father suffering lung cancer also know father kicked hose make re...   \n",
       "44256  wait till watch one mean even reading review movie till date sucked one one thing wont understand ripping english flick add creativity amount spent making movie producers considered buying rights cellular dubbed hindi released movie might gotten profits way guess chance rate movie 0 would done pathetic performances come tanushree datta girl played sidekick aftab know problem seen cellular much earlier movie cant reason support movie could go hours neither time discuss useless crap movie want...   \n",
       "18958  movie possibly cheapest cheesiest poorest sequel ever made br br yet funniest idiotic movie disney guarantee laughs sappy stories lame plots start finish br br group short stories seem like bad fanfictions br br spoiler alert first one beast belle petty pathetic argument three loser new characters decide patch things forging letter forgiveness give belle part way little episode belle wall eyes made siblings laugh hard beast fight letter later learn meaning forgiveness old certainly old enoug...   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            text_tokenized  \\\n",
       "41631  [2, deathly, unfunny, girls, stays, deathly, unfunny, uncle, benny, beach, house, uncle, beeny, like, party, guess, deathly, unfunny, girls, yup, guessed, deathly, unfunny, beach, party, catch, subliminal, message, trying, convey, first, moron, would, rather, watch, nude, jello, tag, team, watching, match, bea, aurther, cameryn, manhiem, vs, rosie, donnell, jessica, tandy, movie, lose, term, loosely, bad, br, br, grade, f, br, br, eye, candy, kristin, novak, charity, rahmer, go, topless, iva...   \n",
       "4372                                                                                                                                                                                                                                             [la, teta, la, luna, symbolic, spain, film, everything, film, occurs, symbolic, meaning, totally, different, usual, movie, one, access, br, br, film, good, good, people, want, look, meaning, everything, film, tale, must, advice, sample, film, br, br, please, enjoy]   \n",
       "3425   [basic, storyline, aditiya, kumar, spoilt, son, millionaire, ishwar, bachan, owns, toy, industry, ishwar, eyes, son, aditya, nothing, wrong, aditya, mother, sumitra, shefali, shah, warns, ishwar, bring, son, responsible, path, late, ishwar, patient, lung, cancer, 9, months, live, son, elopes, marries, mitali, chopra, ishwar, readily, forgives, aditya, happy, couple, aditya, mitali, come, back, honeymoon, mitali, pregnant, forces, ishwar, kick, aditya, house, make, responsible, aditya, know, ...   \n",
       "44256  [wait, till, watch, one, mean, even, reading, review, movie, till, date, sucked, one, one, thing, wont, understand, ripping, english, flick, add, creativity, amount, spent, making, movie, producers, considered, buying, rights, cellular, dubbed, hindi, released, movie, might, gotten, profits, way, guess, chance, rate, movie, 0, would, done, pathetic, performances, come, tanushree, datta, girl, played, sidekick, aftab, know, problem, seen, cellular, much, earlier, movie, cant, reason, support,...   \n",
       "18958  [movie, possibly, cheapest, cheesiest, poorest, sequel, ever, made, br, br, yet, funniest, idiotic, movie, disney, guarantee, laughs, sappy, stories, lame, plots, start, finish, br, br, group, short, stories, seem, like, bad, fanfictions, br, br, spoiler, alert, first, one, beast, belle, petty, pathetic, argument, three, loser, new, characters, decide, patch, things, forging, letter, forgiveness, give, belle, part, way, little, episode, belle, wall, eyes, made, siblings, laugh, hard, beast, ...   \n",
       "\n",
       "       Pos  Neg  \n",
       "41631    0    1  \n",
       "4372     1    0  \n",
       "3425     1    0  \n",
       "44256    0    1  \n",
       "18958    0    1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>text_tokenized</th>\n",
       "      <th>is_movie_related</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>362212</td>\n",
       "      <td>1</td>\n",
       "      <td>heyy live london bdw amazing actor tc x x</td>\n",
       "      <td>[heyy, live, london, bdw, amazing, actor, tc, x, x]</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>936834</td>\n",
       "      <td>1</td>\n",
       "      <td>decided good day keep quiet watch hubby rework veggie gardens</td>\n",
       "      <td>[decided, good, day, keep, quiet, watch, hubby, rework, veggie, gardens]</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84942</td>\n",
       "      <td>1</td>\n",
       "      <td>excellent like floods mail amusing watch come</td>\n",
       "      <td>[excellent, like, floods, mail, amusing, watch, come]</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1007098</td>\n",
       "      <td>1</td>\n",
       "      <td>morning watching family channel 4 braces 8 days yaaaaaay</td>\n",
       "      <td>[morning, watching, family, channel, 4, braces, 8, days, yaaaaaay]</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>602606</td>\n",
       "      <td>0</td>\n",
       "      <td>cant figure get dvd download onto ipod</td>\n",
       "      <td>[cant, figure, get, dvd, download, onto, ipod]</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         label                                                           text  \\\n",
       "362212       1                      heyy live london bdw amazing actor tc x x   \n",
       "936834       1  decided good day keep quiet watch hubby rework veggie gardens   \n",
       "84942        1                  excellent like floods mail amusing watch come   \n",
       "1007098      1       morning watching family channel 4 braces 8 days yaaaaaay   \n",
       "602606       0                         cant figure get dvd download onto ipod   \n",
       "\n",
       "                                                                   text_tokenized  \\\n",
       "362212                        [heyy, live, london, bdw, amazing, actor, tc, x, x]   \n",
       "936834   [decided, good, day, keep, quiet, watch, hubby, rework, veggie, gardens]   \n",
       "84942                       [excellent, like, floods, mail, amusing, watch, come]   \n",
       "1007098        [morning, watching, family, channel, 4, braces, 8, days, yaaaaaay]   \n",
       "602606                             [cant, figure, get, dvd, download, onto, ipod]   \n",
       "\n",
       "         is_movie_related  Pos  Neg  \n",
       "362212               True    1    0  \n",
       "936834               True    1    0  \n",
       "84942                True    1    0  \n",
       "1007098              True    1    0  \n",
       "602606               True    0    1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imdb_pre_processed = pd.read_pickle(\"imdb_pre_processed.pkl\")\n",
    "twitter_pre_processed = pd.read_pickle(\"twitter_pre_processed.pkl\")\n",
    "\n",
    "display(imdb_pre_processed.sample(5))\n",
    "display(twitter_pre_processed.sample(5))\n",
    "\n",
    "#Convert to numpy arrays\n",
    "sentences_imdb = imdb_pre_processed['text'].values\n",
    "y_imdb = imdb_pre_processed['label'].values\n",
    "\n",
    "sentences_twitter = twitter_pre_processed['text'].values\n",
    "y_twitter = twitter_pre_processed['label'].values\n",
    "\n",
    "#Generate a train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "sentences_train_imdb, sentences_test_imdb, y_train_imdb, y_test_imdb = train_test_split(sentences_imdb, \n",
    "                                                                                        y_imdb, test_size=0.25, random_state=47)\n",
    "\n",
    "sentences_train_twitter, sentences_test_twitter, y_train_twitter, y_test_twitter = train_test_split(sentences_twitter, \n",
    "                                                                                        y_twitter, test_size=0.25, random_state=47)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "vectorizer_imdb = CountVectorizer(max_features=10000)\n",
    "vectorizer_imdb.fit(sentences_train_imdb)\n",
    "X_train_imdb = vectorizer_imdb.transform(sentences_train_imdb)\n",
    "X_test_imdb  = vectorizer_imdb.transform(sentences_test_imdb)\n",
    "\n",
    "vectorizer_twitter = CountVectorizer(max_features=10000)\n",
    "vectorizer_twitter.fit(sentences_train_twitter)\n",
    "X_train_twitter = vectorizer_twitter.transform(sentences_train_twitter)\n",
    "X_test_twitter  = vectorizer_twitter.transform(sentences_test_twitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We have decided that we are going to switch to a CNN here instead of a dense neural network. \n",
    "#The reason for this is that there are less weights to optimize, so we can evaluate deeper networks which will hopefully\n",
    "#Give us better performance. The embedding we chose is the google news word2vec pre-trained model. It includes word vectors for a\n",
    "#vocabulary of 3 million words and phrases that they trained on roughly 100 billion words from a google dataset. The vector length\n",
    "#is 300 features. We use gensim to load this word2vec embedding. \n",
    "imdb_pre_processed = pd.read_pickle(\"imdb_pre_processed.pkl\")\n",
    "twitter_pre_processed = pd.read_pickle(\"twitter_pre_processed.pkl\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "imdb_train, imdb_test = train_test_split(imdb_pre_processed, test_size=0.15, random_state=47)\n",
    "twitter_train, twitter_test = train_test_split(twitter_pre_processed, test_size=0.15, random_state=47)\n",
    "\n",
    "\n",
    "# from gensim import models\n",
    "\n",
    "# word2vec_path = 'GoogleNews-vectors-negative300.bin.gz'\n",
    "# word2vec = models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5258596 words total, with a vocabulary size of 94536 (IMDB Train data)\n",
      "Max sentence length is 1455 (IMDB Train data)\n",
      "923208 words total, with a vocabulary size of 46003 (IMDB Test data)\n",
      "Max sentence length is 949 (IMDB Test data)\n",
      "406771 words total, with a vocabulary size of 32440 (Twitter Train data)\n",
      "Max sentence length is 27\n",
      "71707 words total, with a vocabulary size of 11597 (Twitter Test data)\n",
      "Max sentence length is 26 (Twitter Test data)\n"
     ]
    }
   ],
   "source": [
    "#Check length of words and vocabulary size so we can see if our word2vec model works correctly later. \n",
    "#IMDB \n",
    "all_training_words_imdb = [word for tokens in imdb_train[\"text_tokenized\"] for word in tokens]\n",
    "training_sentence_lengths_imdb = [len(tokens) for tokens in imdb_train[\"text_tokenized\"]]\n",
    "TRAINING_VOCAB_IMDB = sorted(list(set(all_training_words_imdb)))\n",
    "print(\"{} words total, with a vocabulary size of {} (IMDB Train data)\".format(len(all_training_words_imdb), len(TRAINING_VOCAB_IMDB)))\n",
    "print(\"Max sentence length is {} (IMDB Train data)\".format(max(training_sentence_lengths_imdb)))\n",
    "\n",
    "all_test_words_imdb = [word for tokens in imdb_test[\"text_tokenized\"] for word in tokens]\n",
    "test_sentence_lengths_imdb = [len(tokens) for tokens in imdb_test[\"text_tokenized\"]]\n",
    "TESTING_VOCAB_IMDB = sorted(list(set(all_test_words_imdb)))\n",
    "print(\"{} words total, with a vocabulary size of {} (IMDB Test data)\".format(len(all_test_words_imdb), len(TESTING_VOCAB_IMDB)))\n",
    "print(\"Max sentence length is {} (IMDB Test data)\".format(max(test_sentence_lengths_imdb)))\n",
    "\n",
    "\n",
    "#TWITTER\n",
    "all_training_words_twitter = [word for tokens in twitter_train[\"text_tokenized\"] for word in tokens]\n",
    "training_sentence_lengths_twitter = [len(tokens) for tokens in twitter_train[\"text_tokenized\"]]\n",
    "TRAINING_VOCAB_TWITTER = sorted(list(set(all_training_words_twitter)))\n",
    "print(\"{} words total, with a vocabulary size of {} (Twitter Train data)\".format(len(all_training_words_twitter), len(TRAINING_VOCAB_TWITTER)))\n",
    "print(\"Max sentence length is {}\".format(max(training_sentence_lengths_twitter)))\n",
    "\n",
    "all_test_words_twitter = [word for tokens in twitter_test[\"text_tokenized\"] for word in tokens]\n",
    "test_sentence_lengths_twitter = [len(tokens) for tokens in twitter_test[\"text_tokenized\"]]\n",
    "TESTING_VOCAB_twitter = sorted(list(set(all_test_words_twitter)))\n",
    "print(\"{} words total, with a vocabulary size of {} (Twitter Test data)\".format(len(all_test_words_twitter), len(TESTING_VOCAB_twitter)))\n",
    "print(\"Max sentence length is {} (Twitter Test data)\".format(max(test_sentence_lengths_twitter)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After the word2vec model, we use a function to generate the embeddings. The get_average_word2vec function\n",
    "#takes a list of tokenized words, vectors is the embedding we will use, and generate_missings is a boolean variable that will generate \n",
    "#a number for the missing words. If the length of a review is <1, we generate a np.array of zeros. \n",
    "def get_average_word2vec(tokenized_words, vector, generate_missing=False, k=300):\n",
    "    if len(tokenized_words)<1:\n",
    "        return np.zeros(k)\n",
    "    if generate_missing:\n",
    "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokenized_words]\n",
    "    else:\n",
    "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokenized_words]\n",
    "    length = len(vectorized)\n",
    "    summed = np.sum(vectorized, axis=0)\n",
    "    averaged = np.divide(summed, length)\n",
    "    return averaged\n",
    "\n",
    "#This function applies the above function to our data. \n",
    "def get_word2vec_embeddings(vectors, clean_comments, generate_missing=False):\n",
    "    embeddings = clean_comments['text_tokenized'].apply(lambda x: get_average_word2vec(x, vectors, \n",
    "                                                                                generate_missing))\n",
    "    return list(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_imdb_embeddings = get_word2vec_embeddings(word2vec, imdb_train, generate_missing=True)\n",
    "training_twitter_embeddings = get_word2vec_embeddings(word2vec, twitter_train, generate_missing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 94536 unique words.\n",
      "Found 32440 unique words.\n"
     ]
    }
   ],
   "source": [
    "#Tokenize the data, and pad the sequences so we have proper input into the CNN. \n",
    "MAX_SEQUENCE_LENGTH = 50\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "#Initialize the Tokenizer, with max words to keep as the numer of unique words in imdb dataset.\n",
    "tokenizer_imdb = Tokenizer(num_words=len(TRAINING_VOCAB_IMDB))\n",
    "tokenizer_twitter = Tokenizer(num_words=len(TRAINING_VOCAB_TWITTER))\n",
    "\n",
    "#Updates internal vocabulary based on a list of texts. This method creates the vocabulary index based on word frequency\n",
    "tokenizer_imdb.fit_on_texts(imdb_train[\"text\"].tolist())\n",
    "tokenizer_twitter.fit_on_texts(twitter_train[\"text\"].tolist())\n",
    "\n",
    "#Transforms each text in texts to a sequence of integers. \n",
    "#So it basically takes each word in the text and replaces it with its corresponding integer value from the word_index dictionary.\n",
    "training_sequences_imdb = tokenizer_imdb.texts_to_sequences(imdb_train[\"text\"].tolist())\n",
    "training_sequences_twitter = tokenizer_twitter.texts_to_sequences(twitter_train[\"text\"].tolist())\n",
    "\n",
    "#Word_index is a dictionary mapping words (str) to their rank/index (int). \n",
    "imdb_train_word_index = tokenizer_imdb.word_index \n",
    "print('Found {} unique words.'.format(len(imdb_train_word_index)))\n",
    "\n",
    "twitter_train_word_index = tokenizer_twitter.word_index\n",
    "print('Found {} unique words.'.format(len(twitter_train_word_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In the output from the question above, we can see that after the word embeddings the unique number of words is still the same!\n",
    "#So that part worked. \n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "train_cnn_imdb_data = pad_sequences(training_sequences_imdb, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "train_cnn_twitter_data = pad_sequences(training_sequences_twitter, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(94537, 300)\n",
      "(32441, 300)\n"
     ]
    }
   ],
   "source": [
    "#Initialize a np.zeros array with the dimensions of our training data. \n",
    "imdb_train_embedding_weights = np.zeros((len(imdb_train_word_index)+1, EMBEDDING_DIM))\n",
    "twitter_train_embedding_weights = np.zeros((len(twitter_train_word_index)+1, EMBEDDING_DIM))\n",
    "\n",
    "#Give the matrix the value of the word2vec embedding if the word is IN the word2vec embedding, \n",
    "#else give it a random number with the shape of EMBEDDING_DIM\n",
    "for word,index in imdb_train_word_index.items():\n",
    "    imdb_train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
    "    \n",
    "for word,index in twitter_train_word_index.items():\n",
    "    twitter_train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
    "    \n",
    "print(imdb_train_embedding_weights.shape)\n",
    "print(twitter_train_embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now training data is properly defined, of course we also need to define the testing data. \n",
    "test_sequences_imdb = tokenizer_imdb.texts_to_sequences(imdb_test[\"text\"].tolist())\n",
    "test_sequences_twitter = tokenizer_twitter.texts_to_sequences(twitter_test[\"text\"].tolist())\n",
    "\n",
    "test_cnn_imdb_data = pad_sequences(test_sequences_imdb, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_cnn_twitter_data = pad_sequences(test_sequences_twitter, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Conv1D, GlobalMaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "\n",
    "def ConvNet_unTuned(embeddings, max_sequence_length, num_words, embedding_dim, labels_index):\n",
    "    \n",
    "    #The embedding layer turns positive integers (indexes) into dense vectors of fixed size.\n",
    "    #It requires that the input data be integer encoded, so that each word is represented by a unique integer. \n",
    "    #Which is what we did previously. \n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            weights=[embeddings],\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=False)\n",
    "    \n",
    "    #Then we specify the shape of the input dimensions\n",
    "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    \n",
    "    #Create an empty convs list where we will create all the convolutional layers and max-pooling layers\n",
    "    convs = []\n",
    "    filter_sizes = [2,3,4,5,6]\n",
    "\n",
    "    #Use a for-loop to declare convolutional layers and pooling layers. \n",
    "    #Every Conv-layer has 200 filters, and we keep increasing the kernel_size by 1 every conv+pooling layer.\n",
    "    #We have no emperical reason for doing it like this, as the choice of deciding how many hidden layers, how many neurons, \n",
    "    #kernel sizes etc. seems to be a very active research area, but there is no \"golden rule\" (yet). Most good setups\n",
    "    #come about through to methodical experimentation, and that is also how we came to this setup \n",
    "    #(With a little help from the Interwebs of course)\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=200, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
    "        l_pool = GlobalMaxPooling1D()(l_conv)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "    #Concatenate all the convolutional/max-pooling layers created in the for-loop above. \n",
    "    l_merge = concatenate(convs, axis=1)\n",
    "    #Add a Dense layer of 128 neurons\n",
    "    x = Dense(128, activation='relu')(l_merge)\n",
    "    #Add the final layer that will do the predictions (Only 2 neurons, 1 for positive 1 for negative)\n",
    "    preds = Dense(labels_index, activation='sigmoid')(x)\n",
    "\n",
    "    #Initialize the model\n",
    "    model = Model(sequence_input, preds)\n",
    "    #Compile it. We use binary_crossentropy because basically this is a binary prediction problem.\n",
    "    #Adam is an efficient implementation of gradient descent, and we ask the model to return accuracy. \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make sure that y is in the right format. \n",
    "label_names = ['Pos', 'Neg']\n",
    "y_train_imdb = imdb_train[label_names].values\n",
    "y_train_twitter = twitter_train[label_names].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1127 10:54:22.345404 18728 deprecation_wrapper.py:119] From C:\\Users\\Yme\\Anaconda3\\envs\\DeepLearning3.6\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1127 10:54:22.393259 18728 deprecation_wrapper.py:119] From C:\\Users\\Yme\\Anaconda3\\envs\\DeepLearning3.6\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1127 10:54:22.403234 18728 deprecation_wrapper.py:119] From C:\\Users\\Yme\\Anaconda3\\envs\\DeepLearning3.6\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1127 10:54:22.428356 18728 deprecation_wrapper.py:119] From C:\\Users\\Yme\\Anaconda3\\envs\\DeepLearning3.6\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W1127 10:54:22.428356 18728 deprecation_wrapper.py:119] From C:\\Users\\Yme\\Anaconda3\\envs\\DeepLearning3.6\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W1127 10:54:26.711674 18728 deprecation_wrapper.py:119] From C:\\Users\\Yme\\Anaconda3\\envs\\DeepLearning3.6\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W1127 10:54:26.745557 18728 deprecation.py:323] From C:\\Users\\Yme\\Anaconda3\\envs\\DeepLearning3.6\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 50, 300)      28361100    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 49, 200)      120200      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 48, 200)      180200      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 47, 200)      240200      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 46, 200)      300200      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 45, 200)      360200      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 200)          0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 200)          0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 200)          0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 200)          0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 200)          0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1000)         0           global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "                                                                 global_max_pooling1d_4[0][0]     \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          128128      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2)            258         dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 29,690,486\n",
      "Trainable params: 1,329,386\n",
      "Non-trainable params: 28,361,100\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 50, 300)      9732300     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 49, 200)      120200      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 48, 200)      180200      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 47, 200)      240200      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 46, 200)      300200      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 45, 200)      360200      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 200)          0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_7 (GlobalM (None, 200)          0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_8 (GlobalM (None, 200)          0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_9 (GlobalM (None, 200)          0           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_10 (Global (None, 200)          0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 1000)         0           global_max_pooling1d_6[0][0]     \n",
      "                                                                 global_max_pooling1d_7[0][0]     \n",
      "                                                                 global_max_pooling1d_8[0][0]     \n",
      "                                                                 global_max_pooling1d_9[0][0]     \n",
      "                                                                 global_max_pooling1d_10[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 128)          128128      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 2)            258         dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 11,061,686\n",
      "Trainable params: 1,329,386\n",
      "Non-trainable params: 9,732,300\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Initialize both CNN's with the right training data, and other parameters defined above. \n",
    "model_imdb = ConvNet_unTuned(imdb_train_embedding_weights, MAX_SEQUENCE_LENGTH, \n",
    "                len(imdb_train_word_index)+1, \n",
    "                EMBEDDING_DIM, len(list(label_names)))\n",
    "\n",
    "model_twitter = ConvNet_unTuned(twitter_train_embedding_weights, MAX_SEQUENCE_LENGTH, \n",
    "                len(twitter_train_word_index)+1, \n",
    "                EMBEDDING_DIM, len(list(label_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 38250 samples, validate on 4250 samples\n",
      "Epoch 1/6\n",
      "38250/38250 [==============================] - 31s 810us/step - loss: 0.4095 - acc: 0.8063 - val_loss: 0.3575 - val_acc: 0.8386\n",
      "Epoch 2/6\n",
      "38250/38250 [==============================] - 26s 691us/step - loss: 0.2676 - acc: 0.8875 - val_loss: 0.3199 - val_acc: 0.8629\n",
      "Epoch 3/6\n",
      "38250/38250 [==============================] - 26s 690us/step - loss: 0.1241 - acc: 0.9555 - val_loss: 0.3809 - val_acc: 0.8595\n",
      "Epoch 4/6\n",
      "38250/38250 [==============================] - 27s 694us/step - loss: 0.0399 - acc: 0.9876 - val_loss: 0.5003 - val_acc: 0.8649\n",
      "Epoch 5/6\n",
      "38250/38250 [==============================] - 27s 697us/step - loss: 0.0299 - acc: 0.9894 - val_loss: 0.5980 - val_acc: 0.8575\n",
      "Epoch 6/6\n",
      "38250/38250 [==============================] - 27s 707us/step - loss: 0.0379 - acc: 0.9861 - val_loss: 0.5042 - val_acc: 0.8649\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 6\n",
    "batch_size = 34\n",
    "\n",
    "#Fit the imdb model, with a validation split of 0.1 so we can see how the model performs during training. \n",
    "final_imdb_model = model_imdb.fit(train_cnn_imdb_data, y_train_imdb, epochs=num_epochs, \n",
    "                                  validation_split=0.1, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40770 samples, validate on 4531 samples\n",
      "Epoch 1/6\n",
      "40770/40770 [==============================] - 31s 759us/step - loss: 0.5228 - acc: 0.7413 - val_loss: 0.5031 - val_acc: 0.7583\n",
      "Epoch 2/6\n",
      "40770/40770 [==============================] - 28s 695us/step - loss: 0.4422 - acc: 0.7964 - val_loss: 0.4935 - val_acc: 0.7693\n",
      "Epoch 3/6\n",
      "40770/40770 [==============================] - 29s 700us/step - loss: 0.3537 - acc: 0.8470 - val_loss: 0.5707 - val_acc: 0.7576\n",
      "Epoch 4/6\n",
      "40770/40770 [==============================] - 28s 695us/step - loss: 0.2322 - acc: 0.9058 - val_loss: 0.6458 - val_acc: 0.7494\n",
      "Epoch 5/6\n",
      "40770/40770 [==============================] - 28s 698us/step - loss: 0.1460 - acc: 0.9438 - val_loss: 0.8564 - val_acc: 0.7442\n",
      "Epoch 6/6\n",
      "40770/40770 [==============================] - 29s 701us/step - loss: 0.1091 - acc: 0.9582 - val_loss: 1.0454 - val_acc: 0.7406\n"
     ]
    }
   ],
   "source": [
    "#Fit the twitter model, again with a validation split of 0.1 for the reason we mentioned above. \n",
    "twitter_model = model_twitter.fit(train_cnn_twitter_data, y_train_twitter, epochs=num_epochs, \n",
    "                                  validation_split=0.1, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 2s 311us/step\n"
     ]
    }
   ],
   "source": [
    "#Now, we evaluate the performance on imdb testing data. \n",
    "predictions_imdb = model_imdb.predict(test_cnn_imdb_data, batch_size = 34, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final testing accuracy on testing data of IMDB 0.8505333333333334\n"
     ]
    }
   ],
   "source": [
    "#Check how The model did, by comparing the predicted sentiment to the actual sentiment. \n",
    "#We do this by taking the max argument of the output of the CNN. The neuron which has the highest output is the\n",
    "#prediction of the CNN, and we set the prediction_labels list to 1 or 0 accordingly (For positive and negative)\n",
    "#then We check how well we did. \n",
    "prediction_labels=[1,0]\n",
    "\n",
    "for p in predictions_imdb:\n",
    "    prediction_labels.append(prediction_labels[np.argmax(p)])\n",
    "    \n",
    "#Pop arbitrary first two elements again\n",
    "prediction_labels.pop(0)\n",
    "prediction_labels.pop(0)\n",
    "    \n",
    "print(\"Final testing accuracy on testing data of IMDB {}\".format(sum(imdb_test['label']==prediction_labels)/len(prediction_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7995/7995 [==============================] - 2s 298us/step\n"
     ]
    }
   ],
   "source": [
    "#Also do this for the performance on twitter testing data\n",
    "predictions_twitter = model_twitter.predict(test_cnn_twitter_data, batch_size=34, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final testing accuracy on testing data of Twitter 0.756722951844903\n"
     ]
    }
   ],
   "source": [
    "prediction_labels=[1,0]\n",
    "\n",
    "for p in predictions_twitter:\n",
    "    prediction_labels.append(prediction_labels[np.argmax(p)])\n",
    "    \n",
    "#Pop arbitrary first two elements again\n",
    "prediction_labels.pop(0)\n",
    "prediction_labels.pop(0)\n",
    "\n",
    "print(\"Final testing accuracy on testing data of Twitter {}\".format(sum(twitter_test['label']==prediction_labels)/len(prediction_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concluding question two, we can see that we have very good accuracy for both the IMDB and the Twitter dataset!Even without hyperparameter tuning. This is nice, since especially considering the twitter dataset also contains a lot of noise (random words, misspelled words, etc) which makes it harder to predict. For IMDB network seems to be overfitting, as the accuracy is almost 100 % on training data, but the testing accuracy remains a bit lower... Maybe in the next question we can apply regularization techniques to make our network generalize better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now do a hyperparameter tuning strategy. The strategy we chose is described in this book:\n",
    "http://neuralnetworksanddeeplearning.com/chap3.html#how_to_choose_a_neural_network's_hyper-parameters\n",
    "Basically, we just keep experimenting, with different learning rates/regularization types in a structured manner.\n",
    "In the previous question we noticed that our network was overfitting. We now try to fix this.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.layers import regularizers\n",
    "\n",
    "def ConvNet_Tuned(embeddings, max_sequence_length, num_words, embedding_dim, labels_index):\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            weights=[embeddings],\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=False)\n",
    "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    \n",
    "    convs = []\n",
    "    filter_sizes = [2,3,4,5,6]\n",
    "\n",
    "    for filter_size in filter_sizes:\n",
    "        #Add l2 regularization in convolution laters\n",
    "        l_conv = Conv1D(filters=200, kernel_size=filter_size, activation='relu', kernel_regularizer=regularizers.l2(0.005))(embedded_sequences)\n",
    "        l_pool = GlobalMaxPooling1D()(l_conv)\n",
    "        convs.append(l_pool)\n",
    " \n",
    "    l_merge = concatenate(convs, axis=1)\n",
    "    #Add Dropout for the convolution layers\n",
    "    x = Dropout(0.1)(l_merge)  \n",
    "    #Add dropout for the dense layers, also do l2 regularization \n",
    "    x = Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.005))(x)\n",
    "    Dropout(0.1)(x)\n",
    "    preds = Dense(labels_index, activation='sigmoid')(x)\n",
    "    model = Model(sequence_input, preds)\n",
    "    #Specify a custom optimizer.\n",
    "    #We experimented with a custom learning rate, and learning rate decay (start with high lr, decrease every epoch)\n",
    "    #But it did not increase performance. \n",
    "    #also we use a special version of the ADAM optimizer, mentioned in the paper \"On the Convergence of Adam and Beyond\"\n",
    "    adam = Adam(amsgrad=True)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_18 (InputLayer)           (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_18 (Embedding)        (None, 50, 300)      28361100    input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_81 (Conv1D)              (None, 49, 200)      120200      embedding_18[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_82 (Conv1D)              (None, 48, 200)      180200      embedding_18[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_83 (Conv1D)              (None, 47, 200)      240200      embedding_18[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_84 (Conv1D)              (None, 46, 200)      300200      embedding_18[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_85 (Conv1D)              (None, 45, 200)      360200      embedding_18[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_81 (Global (None, 200)          0           conv1d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_82 (Global (None, 200)          0           conv1d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_83 (Global (None, 200)          0           conv1d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_84 (Global (None, 200)          0           conv1d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_85 (Global (None, 200)          0           conv1d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 1000)         0           global_max_pooling1d_81[0][0]    \n",
      "                                                                 global_max_pooling1d_82[0][0]    \n",
      "                                                                 global_max_pooling1d_83[0][0]    \n",
      "                                                                 global_max_pooling1d_84[0][0]    \n",
      "                                                                 global_max_pooling1d_85[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_29 (Dropout)            (None, 1000)         0           concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_33 (Dense)                (None, 128)          128128      dropout_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_34 (Dense)                (None, 2)            258         dense_33[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 29,690,486\n",
      "Trainable params: 1,329,386\n",
      "Non-trainable params: 28,361,100\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_19 (InputLayer)           (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_19 (Embedding)        (None, 50, 300)      9732300     input_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_86 (Conv1D)              (None, 49, 200)      120200      embedding_19[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_87 (Conv1D)              (None, 48, 200)      180200      embedding_19[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_88 (Conv1D)              (None, 47, 200)      240200      embedding_19[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_89 (Conv1D)              (None, 46, 200)      300200      embedding_19[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_90 (Conv1D)              (None, 45, 200)      360200      embedding_19[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_86 (Global (None, 200)          0           conv1d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_87 (Global (None, 200)          0           conv1d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_88 (Global (None, 200)          0           conv1d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_89 (Global (None, 200)          0           conv1d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_90 (Global (None, 200)          0           conv1d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 1000)         0           global_max_pooling1d_86[0][0]    \n",
      "                                                                 global_max_pooling1d_87[0][0]    \n",
      "                                                                 global_max_pooling1d_88[0][0]    \n",
      "                                                                 global_max_pooling1d_89[0][0]    \n",
      "                                                                 global_max_pooling1d_90[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_31 (Dropout)            (None, 1000)         0           concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_35 (Dense)                (None, 128)          128128      dropout_31[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_36 (Dense)                (None, 2)            258         dense_35[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 11,061,686\n",
      "Trainable params: 1,329,386\n",
      "Non-trainable params: 9,732,300\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Initialize both CNN's with the right training data, and other parameters defined above. \n",
    "model_imdb_tuned = ConvNet_Tuned(imdb_train_embedding_weights, MAX_SEQUENCE_LENGTH, \n",
    "                len(imdb_train_word_index)+1, \n",
    "                EMBEDDING_DIM, len(list(label_names)))\n",
    "\n",
    "model_twitter_tuned = ConvNet_Tuned(twitter_train_embedding_weights, MAX_SEQUENCE_LENGTH, \n",
    "                len(twitter_train_word_index)+1, \n",
    "                EMBEDDING_DIM, len(list(label_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 38250 samples, validate on 4250 samples\n",
      "Epoch 1/15\n",
      "38250/38250 [==============================] - 30s 788us/step - loss: 0.8476 - acc: 0.7709 - val_loss: 0.5599 - val_acc: 0.8122\n",
      "Epoch 2/15\n",
      "38250/38250 [==============================] - 29s 749us/step - loss: 0.5361 - acc: 0.8184 - val_loss: 0.4899 - val_acc: 0.8396\n",
      "Epoch 3/15\n",
      "38250/38250 [==============================] - 29s 750us/step - loss: 0.5041 - acc: 0.8264 - val_loss: 0.4646 - val_acc: 0.8488\n",
      "Epoch 4/15\n",
      "38250/38250 [==============================] - 29s 750us/step - loss: 0.4850 - acc: 0.8346 - val_loss: 0.4609 - val_acc: 0.8481\n",
      "Epoch 5/15\n",
      "38250/38250 [==============================] - 29s 752us/step - loss: 0.4753 - acc: 0.8369 - val_loss: 0.4714 - val_acc: 0.8400\n",
      "Epoch 6/15\n",
      "38250/38250 [==============================] - 29s 750us/step - loss: 0.4680 - acc: 0.8402 - val_loss: 0.4507 - val_acc: 0.8512\n",
      "Epoch 7/15\n",
      "38250/38250 [==============================] - 29s 752us/step - loss: 0.4583 - acc: 0.8434 - val_loss: 0.4407 - val_acc: 0.8516\n",
      "Epoch 8/15\n",
      "38250/38250 [==============================] - 29s 750us/step - loss: 0.4554 - acc: 0.8451 - val_loss: 0.4386 - val_acc: 0.8521\n",
      "Epoch 9/15\n",
      "38250/38250 [==============================] - 29s 752us/step - loss: 0.4493 - acc: 0.8481 - val_loss: 0.4399 - val_acc: 0.8482\n",
      "Epoch 10/15\n",
      "38250/38250 [==============================] - 29s 752us/step - loss: 0.4448 - acc: 0.8508 - val_loss: 0.4399 - val_acc: 0.8506\n",
      "Epoch 11/15\n",
      "38250/38250 [==============================] - 29s 748us/step - loss: 0.4431 - acc: 0.8529 - val_loss: 0.4292 - val_acc: 0.8567\n",
      "Epoch 12/15\n",
      "38250/38250 [==============================] - 29s 753us/step - loss: 0.4435 - acc: 0.8513 - val_loss: 0.4431 - val_acc: 0.8541\n",
      "Epoch 13/15\n",
      "38250/38250 [==============================] - 29s 751us/step - loss: 0.4387 - acc: 0.8531 - val_loss: 0.4312 - val_acc: 0.8586\n",
      "Epoch 14/15\n",
      "38250/38250 [==============================] - 29s 752us/step - loss: 0.4370 - acc: 0.8558 - val_loss: 0.4316 - val_acc: 0.8572\n",
      "Epoch 15/15\n",
      "38250/38250 [==============================] - 29s 752us/step - loss: 0.4352 - acc: 0.8569 - val_loss: 0.4324 - val_acc: 0.8533\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 15\n",
    "batch_size = 34\n",
    "\n",
    "#Fit the models again, we also increase the number of epochs \n",
    "final_imdb_model = model_imdb_tuned.fit(train_cnn_imdb_data, y_train_imdb, epochs=num_epochs, \n",
    "                                  validation_split=0.1, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40770 samples, validate on 4531 samples\n",
      "Epoch 1/15\n",
      "40770/40770 [==============================] - 33s 816us/step - loss: 0.8133 - acc: 0.7242 - val_loss: 0.6261 - val_acc: 0.7216\n",
      "Epoch 2/15\n",
      "40770/40770 [==============================] - 31s 753us/step - loss: 0.5810 - acc: 0.7498 - val_loss: 0.5821 - val_acc: 0.7408\n",
      "Epoch 3/15\n",
      "40770/40770 [==============================] - 31s 748us/step - loss: 0.5620 - acc: 0.7564 - val_loss: 0.5799 - val_acc: 0.7349\n",
      "Epoch 4/15\n",
      "40770/40770 [==============================] - 30s 746us/step - loss: 0.5533 - acc: 0.7563 - val_loss: 0.5630 - val_acc: 0.7483\n",
      "Epoch 5/15\n",
      "40770/40770 [==============================] - 30s 748us/step - loss: 0.5494 - acc: 0.7606 - val_loss: 0.5662 - val_acc: 0.7505\n",
      "Epoch 6/15\n",
      "40770/40770 [==============================] - 30s 745us/step - loss: 0.5457 - acc: 0.7624 - val_loss: 0.5523 - val_acc: 0.7577\n",
      "Epoch 7/15\n",
      "40770/40770 [==============================] - 31s 748us/step - loss: 0.5432 - acc: 0.7644 - val_loss: 0.5574 - val_acc: 0.7519\n",
      "Epoch 8/15\n",
      "40770/40770 [==============================] - 30s 746us/step - loss: 0.5416 - acc: 0.7649 - val_loss: 0.5617 - val_acc: 0.7506\n",
      "Epoch 9/15\n",
      "40770/40770 [==============================] - 31s 749us/step - loss: 0.5382 - acc: 0.7680 - val_loss: 0.5544 - val_acc: 0.7530\n",
      "Epoch 10/15\n",
      "40770/40770 [==============================] - 30s 747us/step - loss: 0.5373 - acc: 0.7689 - val_loss: 0.5529 - val_acc: 0.7594\n",
      "Epoch 11/15\n",
      "40770/40770 [==============================] - 30s 744us/step - loss: 0.5351 - acc: 0.7702 - val_loss: 0.5519 - val_acc: 0.7565\n",
      "Epoch 12/15\n",
      "40770/40770 [==============================] - 31s 750us/step - loss: 0.5357 - acc: 0.7697 - val_loss: 0.5537 - val_acc: 0.7589\n",
      "Epoch 13/15\n",
      "40770/40770 [==============================] - 31s 748us/step - loss: 0.5339 - acc: 0.7725 - val_loss: 0.5554 - val_acc: 0.7474\n",
      "Epoch 14/15\n",
      "40770/40770 [==============================] - 31s 752us/step - loss: 0.5338 - acc: 0.7707 - val_loss: 0.5537 - val_acc: 0.7566\n",
      "Epoch 15/15\n",
      "40770/40770 [==============================] - 31s 751us/step - loss: 0.5321 - acc: 0.7729 - val_loss: 0.5576 - val_acc: 0.7556\n"
     ]
    }
   ],
   "source": [
    "#Fit the models again, we also increase the number of epochs \n",
    "twitter_model = model_twitter_tuned.fit(train_cnn_twitter_data, y_train_twitter, epochs=num_epochs, \n",
    "                                  validation_split=0.1, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 8s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "#Now, we evaluate the performance on imdb testing data. \n",
    "predictions_imdb = model_imdb_tuned.predict(test_cnn_imdb_data, batch_size = 34, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final testing accuracy on testing data of IMDB on tuned CNN 0.8397333333333333\n",
      "{0, 1}\n"
     ]
    }
   ],
   "source": [
    "prediction_labels_imdb=[1,0]\n",
    "\n",
    "for p in predictions_imdb:\n",
    "    prediction_labels_imdb.append(prediction_labels_imdb[np.argmax(p)])\n",
    "    \n",
    "#Pop arbitrary first two elements again\n",
    "prediction_labels_imdb.pop(0)\n",
    "prediction_labels_imdb.pop(0)\n",
    "    \n",
    "print(\"Final testing accuracy on testing data of IMDB on tuned CNN {}\".format(sum(imdb_test['label']==prediction_labels_imdb)/len(prediction_labels_imdb)))\n",
    "\n",
    "with open('prediction_labels_imdb.pkl', 'wb') as f:                           \n",
    "    pickle.dump(prediction_labels_imdb, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7995/7995 [==============================] - 2s 230us/step\n"
     ]
    }
   ],
   "source": [
    "#Also do this for the performance on twitter testing data\n",
    "predictions_twitter = model_twitter_tuned.predict(test_cnn_twitter_data, batch_size=34, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final testing accuracy on testing data of Twitter on Tuned CNN 0.7697310819262039\n"
     ]
    }
   ],
   "source": [
    "prediction_labels_twitter=[1,0]\n",
    "\n",
    "for p in predictions_twitter:\n",
    "    prediction_labels_twitter.append(prediction_labels_twitter[np.argmax(p)])\n",
    "    \n",
    "#Pop arbitrary first two elements again\n",
    "prediction_labels_twitter.pop(0)\n",
    "prediction_labels_twitter.pop(0)\n",
    "\n",
    "print(\"Final testing accuracy on testing data of Twitter on Tuned CNN {}\".format(sum(twitter_test['label']==prediction_labels_twitter)/len(prediction_labels_twitter)))\n",
    "\n",
    "with open('prediction_labels_twitter.pkl', 'wb') as f:                           \n",
    "    pickle.dump(prediction_labels_twitter, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setups we tried to vary:\n",
    "- Learning rate (We started @ 0.01 with learning rate decay where decay = lr/25, and kept increasing by 0.1 untill we reached an initial lr of 1.).\n",
    "- We tried to vary the l2 regularization, the following lambda's were tried 0.005, 0.1, 0,25. This does reduce overfit but it does not increase validation/testing accuracy.\n",
    "- We tried to add dropout layers. This also reduces overfit but not validation accuracy. \n",
    "\n",
    "\n",
    "Summarizing, even though the regularization methods do seem to reduce overfit, they do not seem to increase validation accuracy. We think the network with the optimized hyperparameters will generalize better in \"the real world\" as it is les fitted to training data and has roughly the same validation/testing accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, we get the output activations from the 14th layer (which is the last dense layer before the output layer with 2 neurons)\n",
    "#While browsing stackoverflow on how to achieve this (https://stackoverflow.com/questions/41711190/keras-how-to-get-the-output-of-each-layer)\n",
    "#we found a very neat tool that does this for us. (https://github.com/philipperemy/keract).\n",
    "#The activations are stored to an ordered dict. If you only want a specific layer (which is the case for us),\n",
    "#You can specify a layer name. \n",
    "from keract import get_activations\n",
    "import pickle\n",
    "\n",
    "model_imdb_tuned.summary() #Here we see we need the layer called 'dense_1'\n",
    "train_activations_imdb = get_activations(model_imdb_tuned, train_cnn_imdb_data, layer_name='dense_1')\n",
    "test_activations_imdb = get_activations(model_imdb_tuned, test_cnn_imdb_data, layer_name = 'dense_1')\n",
    "\n",
    "model_twitter_tuned.summary() #Here we see we need the layer called 'dense_3'\n",
    "train_activations_twitter = get_activations(model_twitter_tuned, train_cnn_twitter_data, layer_name='dense_3')\n",
    "test_activations_twitter = get_activations(model_twitter_tuned, train_cnn_twitter_data, layer_name='dense_3')\n",
    "\n",
    "with open('train_activations_imdb.pkl', 'wb') as f:                           \n",
    "    pickle.dump(train_activations_imdb, f)\n",
    "    \n",
    "with open('test_activations_imdb.pkl', 'wb') as f:                          \n",
    "    pickle.dump(test_activations_imdb, f)\n",
    "    \n",
    "with open('train_activations_twitter.pkl', 'wb') as f:                           \n",
    "    pickle.dump(train_activations_twitter, f)\n",
    "\n",
    "with open('train_activations_twitter.pkl', 'wb') as f:                         \n",
    "    pickle.dump(train_activations_twitter, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['dense_1/Relu:0'])\n",
      "odict_keys(['dense_3/Relu:0'])\n",
      "42500\n",
      "7500\n",
      "45301\n",
      "7995\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('train_activations_imdb.pkl', 'rb') as f:\n",
    "    train_activations_imdb_loaded = pickle.load(f)\n",
    "    \n",
    "with open('test_activations_imdb.pkl', 'rb') as f:\n",
    "    test_activations_imdb_loaded = pickle.load(f)\n",
    "\n",
    "with open('train_activations_twitter.pkl', 'rb') as f:\n",
    "    train_activations_twitter_loaded = pickle.load(f)\n",
    "    \n",
    "with open('test_activations_twitter.pkl', 'rb') as f:\n",
    "    test_activations_twitter_loaded = pickle.load(f)\n",
    "\n",
    "#Check if the dictionary entries have the right lengths (and they do!) We can use this representation as input for\n",
    "#other 'classical' ML models. \n",
    "print(train_activations_imdb_loaded.keys())\n",
    "print(train_activations_twitter_loaded.keys())\n",
    "\n",
    "print(len(train_activations_imdb_loaded['dense_1/Relu:0']))\n",
    "print(len(test_activations_imdb_loaded['dense_1/Relu:0']))\n",
    "print(len(train_activations_twitter_loaded['dense_3/Relu:0']))\n",
    "print(len(test_activations_twitter_loaded['dense_3/Relu:0']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The IMDB Support vector machine has a classification accuracy of 0.8474666666666667\n",
      "The Twitter Support vector machine has a classification accuracy of 0.7682301438399\n"
     ]
    }
   ],
   "source": [
    "#We chose SVM and Randomforest to evaluate this new representation on. The sole reason for this is that they converge quickly =)\n",
    "from sklearn import svm\n",
    "\n",
    "#Initialize default Support vector machine classifier\n",
    "svc_imdb = svm.SVC() \n",
    "# #Fit the SVC on newly represented train data\n",
    "svc_imdb.fit(train_activations_imdb_loaded['dense_1/Relu:0'], imdb_train['label']) \n",
    "#Make the model predict on test data \n",
    "svc_imdb.predict(test_activations_imdb_loaded['dense_1/Relu:0'])\n",
    "#See how well the model did \n",
    "imdb_score = svc_imdb.score(test_activations_imdb_loaded['dense_1/Relu:0'], imdb_test['label'])\n",
    "print(\"The IMDB Support vector machine has a classification accuracy of {}\".format(imdb_score))\n",
    "\n",
    "# Same story, now for twitter.\n",
    "svc_twitter = svm.SVC() \n",
    "svc_twitter.fit(train_activations_twitter_loaded['dense_3/Relu:0'], twitter_train['label']) \n",
    "svc_imdb.predict(test_activations_twitter_loaded['dense_3/Relu:0'])\n",
    "twitter_score = svc_twitter.score(test_activations_twitter_loaded['dense_3/Relu:0'], twitter_test['label'])\n",
    "print(\"The Twitter Support vector machine has a classification accuracy of {}\".format(twitter_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yme\\Anaconda3\\envs\\DeepLearning3.6\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Twitter Random Forest classifier has a classification accuracy of 0.8413333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yme\\Anaconda3\\envs\\DeepLearning3.6\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Random Forest classifier has a classification accuracy of 0.7403377110694184\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Initialize default RandomForestclassifier\n",
    "rf_imdb = RandomForestClassifier()\n",
    "#Fit the randomforest on newly represented testing data\n",
    "rf_imdb.fit(train_activations_imdb_loaded['dense_1/Relu:0'], imdb_train['label'])\n",
    "#Make the model predict on test data\n",
    "rf_imdb.predict(test_activations_imdb_loaded['dense_1/Relu:0'])\n",
    "#See how well the model did \n",
    "imdb_score = rf_imdb.score(test_activations_imdb_loaded['dense_1/Relu:0'], imdb_test['label'])\n",
    "print(\"The Twitter Random Forest classifier has a classification accuracy of {}\".format(imdb_score))\n",
    "\n",
    "# #Same story, now for twitter\n",
    "rf_imdb = RandomForestClassifier()\n",
    "rf_imdb.fit(train_activations_twitter_loaded['dense_3/Relu:0'], twitter_train['label'])\n",
    "rf_imdb.predict(test_activations_twitter_loaded['dense_3/Relu:0'])\n",
    "twitter_score = rf_imdb.score(test_activations_twitter_loaded['dense_3/Relu:0'], twitter_test['label'])\n",
    "print(\"The Random Forest classifier has a classification accuracy of {}\".format(twitter_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the models predict with roughly the same accuracy as the CNN's evaluated before. Therefore we conclude that the transfer learning step did not make a very big difference. However, it might be easier to train/optimize on this data since both the RandomForest and the SVM train MUCH faster and have less hyperparameters to optimize than the CNN. So what could be a good strategy is, run the CNN once, then extract the new representation, and use this data to fully optimize a SVM or a Randomforest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why we went above and beyond\n",
    "In this assignment we chose to do a sentiment analysis, not for one but for two datasets (Twitter and IMDB). We have learned a lot, even though it was not easy. We discovered that natural language processing is a field so vast one can give a complete course about it. Working with text-data requires a lot of pre-processing, and this was a hard task for us to implement. Especially cleaning the twitter data of garbage like links, HTML-tags, punctuation etc. cost us a lot of time. Then we had to create a proper embedding. This also was something we had never done before, and we went for the google word2vec embedding, a pre-trained vocabulary of 300-dimensional vectors for 3 million (!!!) words and phrases. Also, we were able to train a very deep CNN (14 hidden layers in the end!). This is mainly because something you can not see directly in the notebook; we managed to train all the deep neural networks very fast on a local GPU (Nvidia Quadro M1200). We managed to do this by using the cuDNN library.  The NVIDIA CUDA® Deep Neural Network library (cuDNN) is a GPU-accelerated library of primitives for deep neural networks. cuDNN provides highly tuned implementations for standard routines such as forward and backward convolution, pooling, normalization, and activation layers. We know we could also just use Colab which also has GPU's available, but we think it is cool to mention that we succeeded in setting this up locally. In question 4, we used a very quick way to extract the output activations from the forelast hidden layer. It is called keract, and you should definitely check it out! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
